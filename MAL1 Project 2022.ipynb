{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc732fc",
   "metadata": {},
   "source": [
    "# Machine Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Moskalchuk 293172\n",
    "# Julia Tankiewicz 293719\n",
    "# Nikita Bondarchuk 294478"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edc094",
   "metadata": {},
   "source": [
    "The assignments below should be solved and documented as a project that will form the basis for the\n",
    "examination. When solving the exercises it is important that you\n",
    "\n",
    "  * document all relevant results and analyses that you have obtained/performed during the exercises.\n",
    "  * try to relate your results to the theoretical background of the methods being applied.\n",
    "\n",
    "Feel free to add cells if you need to.\n",
    "\n",
    "Please hand in assignment 1-6 in a _**single**_ Jupyter notebook where you retain the questions outlined below. You are welcome to adapt code from the web (e.g. Kaggle kernels), but you **_must_** reference the original source in your notebook. In addition to _clean, well-documented code_ (i.e. functions with <a href=\"https://www.geeksforgeeks.org/python-docstrings/\">docstrings</a>, etc), your notebook will be judged according to how well each step is explained (using Markdown). \n",
    "\n",
    "In general, direct questions regarding assignments 1, 4, 5 and 6 to Frederik, and questions regarding assignments 2, 3, and 7 to Richard. \n",
    "\n",
    "Last, but not least:\n",
    "* Looking for an overview of the markdown language? The cheat sheet <a href=\"https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\">here</a> might help.\n",
    "* For the Python specific components of the exercises, you should not need constructs beyond those that are already included in the notebooks on the course's web-page (still you should not feel constrained by these, so feel free to be adventurous). You may, however, need to consult the documentation for some of the methods supplied by `sklearn`.\n",
    "\n",
    "**Groups:** Create your own groups. May be across teams. 2-4 students per group. No one-person groups.\n",
    "\n",
    "\n",
    "**Submission deadline:** Thursday, December 15 before 13.00 CET (Notebooks + presentation recording)\n",
    "\n",
    "**Expected workload:** Each student is expected to spend around around 50 hours on the project.\n",
    "\n",
    "### Deliverables\n",
    "The teams have to submit three deliverables before the submission deadline: 1) a notebook of assignments 1-6, 2) a notebook of assignment 7, and 3) presentation video uploaded to some online platform e.g. YouTube, Vimeo, etc.\n",
    "\n",
    "#### Notebook\n",
    "The notebook contains all the code to explore the dataset, train the final model and documents each step clearly. If code is copied from another codebase such as Github or Stack Overflow it **_must_** be properly referenced.\n",
    "\n",
    "\n",
    "#### Presentation\n",
    "The presentation video should be 15 min long and should highlight the problem you are solving, interesting things you found in the data and the step involved in building up your model. At the exam we will discuss the presentation and ask questions about your project and submissions. A link to the video must be placed in the notebook for assignment 7.\n",
    "\n",
    "### Randomness\n",
    "For ALL random states, choose state = 69 so we can replicate your work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c9417",
   "metadata": {},
   "source": [
    "## 1. The IceCat Dataset\n",
    "\n",
    "__You should be able to do this exercise after Lecture 3.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7bcb69",
   "metadata": {},
   "source": [
    "The IceCat Dataset, kindly provided to us by Stibo Systems, contains a large amount of data on different office products. As an example of \"real-world\" data, these data are imperfect and incomplete. As such, this exercise is not so much an exercise in creating a good machine learning model, but places a larger emphasis on \"cleaning the data\".\n",
    "\n",
    "We are going to work with a subset of the IceCat Dataset. In particular, you will be provided with a zip file of 5,854 images of office products, each with the name \"product ID\".jpg. You will also be provided with a list of colors, `colors.txt`, which, when imported using the code below, is a list of tuples of the form `[(\"product ID\", \"color\"), ...]`. (The code below assumes that `colors.txt` is in the same folder as the jupyter notebook. Feel free to change the code if you prefer a different organization of your files).\n",
    "\n",
    "Your task is to clean up the data and construct a simple machine learning model (_e.g._, _k_-nearest neighbor) that can identify the color of a product. You have free hands - there is hardly any one \"correct answer\" - but you need to argue for your choices. Among other things, you probably need to think about the following as you work with the data:\n",
    "\n",
    "* All of the images have different sizes.\n",
    "\n",
    "* Some of the images are RGB images (3 layers), others are CMYK (4 layers), some might even be black-and-white (1 layer).\n",
    "\n",
    "* Some colors are only represented by very few products.\n",
    "\n",
    "* Some colors are very similar, such as \"Purple\" and \"Violet\".\n",
    "\n",
    "* A product may have a particular color, but a packaging of a different color. Similarly, the color of, say, a computer monitor may be black, while the image of it could show a monitor that is turned on with a green screensaver.\n",
    "\n",
    "* Many products are attributed to several colors, such as \"Black, Blue\" or even \"Blue, Green, Orange, Violet, Yellow\". Yet others are described as \"Multicolor\" or \"Assorted colors\".\n",
    "\n",
    "Again, you have free hands in how you are going to solve these (and other) challenges, but you must argue for and reflect on your choices as you progress.\n",
    "\n",
    "\n",
    " ___---GLOBAL VARIABLES---___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "1f92faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary modules here:\n",
    "#pip install opencv-python\n",
    "#pip install keras\n",
    "#pip install --no-cache-dir tensorflow\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import ast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.preprocessing import image\n",
    "#from tensorflow.keras.utils import load_img, img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "e562cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mappings = {\n",
    "  # source color => arr of colors associated with the source color\n",
    "    'Grey' : ['Silver', 'Metallic', 'Aluminium', 'Stainless steel', 'Platinum', 'Graphite', 'Chrome', 'Anthracite','Light Grey','Charcoal','Titanium','Light grey'],\n",
    "    'Purple' : ['Violet', 'Lavender', 'Fuchsia','Magenta'],\n",
    "    'Transparent' : ['Translucent'],\n",
    "    'Cream' : ['Beige', 'Sand', 'Tan','Cappuccino','Ivory'],\n",
    "    'Brown' : ['Oak colour', 'Wood','Bronze'],\n",
    "    'Cyan' : ['Turquoise', 'Aqua colour'],\n",
    "    'White' : ['Pearl' ],\n",
    "    'Pink' : ['Rose'],\n",
    "    'Red' : ['Cherry', 'Bordeaux'],\n",
    "    'Green' : ['Pine', 'Lime','Olive'],\n",
    "    'Yellow' : ['Gold'],\n",
    "    'Blue' : ['Navy'],\n",
    "    'Multicolour' : ['Multi','Assorted colours']\n",
    "}\n",
    "\n",
    "inputDataMock = {\n",
    "    'img_id': [],\n",
    "    'red1': [], 'green1': [], 'blue1': [],\n",
    "    'red2': [], 'green2': [], 'blue2': [],\n",
    "    'red3': [], 'green3': [], 'blue3': [],\n",
    "    'red4': [], 'green4': [], 'blue4': [],\n",
    "    'red5': [], 'green5': [], 'blue5': [],\n",
    "    'color': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e29af8",
   "metadata": {},
   "source": [
    "___---METHODS---___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "90e202dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the source color in response to the associated color using the color_mappings\n",
    "def colors_binning(initial_color):\n",
    "    for source_color in color_mappings:\n",
    "        if initial_color in color_mappings[source_color]: return source_color;\n",
    "    return initial_color;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "7ceab872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dataframe from colors.txt\n",
    "def getCorrectAnswersDF():\n",
    "    with open(\"datasets/1/colors.txt\",\"r\") as file:\n",
    "        colors = ast.literal_eval(file.read())\n",
    "        # lets create a dataframe\n",
    "        d = {'pic_id': [], 'color': []}\n",
    "        for row in colors:\n",
    "            # check if there are more colors in one row\n",
    "            colors_arr = row[1].split(\", \")\n",
    "            d['pic_id'].append(row[0])\n",
    "            d['color'].append(colors_binning(colors_arr[0])) \n",
    "\n",
    "        pic_colors = pd.DataFrame(data=d)\n",
    "        return pic_colors;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "016a70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImgDataSorted(imgData, ignoreWhite = True, colorRound = 2, whiteThreshold = 0.15):\n",
    "    pixel_arr = {}\n",
    "    white = 1 - whiteThreshold;\n",
    "    # itearate over images rows\n",
    "    for imgRow in imgData:\n",
    "        # iterate over pixels\n",
    "        for pixelData in imgRow:\n",
    "            red = round(pixelData.item(0) / 255, colorRound)\n",
    "            green = round(pixelData.item(1) / 255, colorRound)\n",
    "            blue = round(pixelData.item(2) / 255, colorRound)\n",
    "            \n",
    "            # skip white pixels\n",
    "            if (ignoreWhite and red >= white and green >= white and blue >= white): \n",
    "                continue;\n",
    "                \n",
    "            key = str(red) +' '+ str(green) +' '+ str(blue);\n",
    "            # if this pixel color is found in pixel_arr\n",
    "            if (pixel_arr.get(key)): \n",
    "                pixel_arr[key]['count'] += 1 # add 1 to the counter\n",
    "            else: # insert the color, set the counter to 1\n",
    "                pixel_arr[key] = {'R': red, 'G': green, 'B': blue, 'count': 1};\n",
    "    # sort the pixels by number of occurencies and return\n",
    "    return sorted(pixel_arr.values(), key=lambda x: x['count'], reverse=True)\n",
    "\n",
    "def PopulateData(pic_id, pixel_arr, correctAnswer):      \n",
    "    # move the data to the dataFrame mock\n",
    "    inputDataMock['img_id'].append(pic_id)\n",
    "    \n",
    "    for i in range(1,6):\n",
    "        try:\n",
    "            color = pixel_arr[i-1]\n",
    "        except IndexError: # Index out of range - no more colors found\n",
    "            color = {'R': 1.0, 'G': 1.0, 'B': 1.0}\n",
    "            \n",
    "        inputDataMock['red'+str(i)].append(color['R'])\n",
    "        inputDataMock['green'+str(i)].append(color['G'])\n",
    "        inputDataMock['blue'+str(i)].append(color['B'])\n",
    "    \n",
    "    for row_id, row in correctAnswer.iterrows():\n",
    "        col = str(row['color']).strip()\n",
    "        if len(col) == 0: col = None;\n",
    "        \n",
    "        inputDataMock['color'].append(col)\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "9d4fd3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAccuracy(X_train, y_train, X_test, y_test):#lets see how accuracy depends on number of neighbours\n",
    "    #training_accuracy = []\n",
    "    #test_accuracy = []\n",
    "    diff_arr = []\n",
    "    # try n_neighbors from 1 to 10\n",
    "    neighbors_settings = range(3, 25)\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # build the model\n",
    "        clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        clf.fit(X_train, y_train)\n",
    "        # record training set accuracy\n",
    "        train = clf.score(X_train, y_train)\n",
    "        test = clf.score(X_test, y_test)\n",
    "        diff = (train - test) / ((train + test) * 0.5)\n",
    "        #training_accuracy.append(train)\n",
    "        # record generalization accuracy\n",
    "        #test_accuracy.append(test)\n",
    "        diff_arr.append(diff)\n",
    "\n",
    "    #plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "    #plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "    plt.plot(neighbors_settings, diff_arr, label=\"diff\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"n_neighbors\")\n",
    "    plt.legend(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "98a2645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img, dimension=(64, 64)):\n",
    "    img_resized = cv.resize(img, dimension, interpolation=cv.INTER_AREA)\n",
    "    return img_resized\n",
    "\n",
    "def normalize_img(img):\n",
    "    if img.shape == (64, 64):\n",
    "        img = cv.cvtColor(img.astype('float32'),cv.COLOR_GRAY2RGB)\n",
    "    elif img.shape == (64, 64, 4):\n",
    "        img = cv.cvtColor(img.astype('float32'),cv.COLOR_RGBA2RGB)\n",
    "\n",
    "    return img\n",
    "\n",
    "def compress_img(img, nr_of_colors = 16):\n",
    "    \n",
    "    # Lets compress the image using clustering\n",
    "    # https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_ml/py_kmeans/py_kmeans_opencv/py_kmeans_opencv.html\n",
    "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "    flags = cv.KMEANS_PP_CENTERS\n",
    "    compactness, labels, centers = cv.kmeans(np.float32(img.reshape((-1, 3))),\n",
    "                                    nr_of_colors, None, criteria, 10, flags)\n",
    "    centers = np.uint8(centers)\n",
    "    new_colors = centers[labels.flatten()]\n",
    "    img_noramlized = new_colors.reshape(img.shape)\n",
    "    return img_noramlized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "7fddeaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>red1</th>\n",
       "      <th>green1</th>\n",
       "      <th>blue1</th>\n",
       "      <th>red2</th>\n",
       "      <th>green2</th>\n",
       "      <th>blue2</th>\n",
       "      <th>red3</th>\n",
       "      <th>green3</th>\n",
       "      <th>blue3</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26328635</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.12</td>\n",
       "      <td>Brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>802113</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>Grey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29231293</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15764157</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.38</td>\n",
       "      <td>Green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76755969</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.24</td>\n",
       "      <td>Multicolour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5124</th>\n",
       "      <td>71527321</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5125</th>\n",
       "      <td>9484180</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.33</td>\n",
       "      <td>Red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5126</th>\n",
       "      <td>7932471</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.44</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5127</th>\n",
       "      <td>71527335</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5128</th>\n",
       "      <td>30125442</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.69</td>\n",
       "      <td>Multicolour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5129 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        img_id  red1  green1  blue1  red2  green2  blue2  red3  green3  blue3  \\\n",
       "0     26328635  0.53    0.25   0.13  0.57    0.29   0.16  0.49    0.22   0.12   \n",
       "1       802113  0.82    0.81   0.80  0.74    0.74   0.75  0.63    0.63   0.63   \n",
       "2     29231293  0.18    0.17   0.19  0.23    0.22   0.23  0.30    0.29   0.31   \n",
       "3     15764157  0.00    0.54   0.31  0.71    0.71   0.69  0.02    0.63   0.38   \n",
       "4     76755969  0.45    0.46   0.54  0.65    0.84   0.95  0.02    0.14   0.24   \n",
       "...        ...   ...     ...    ...   ...     ...    ...   ...     ...    ...   \n",
       "5124  71527321  0.84    0.85   0.85  1.00    1.00   1.00  1.00    1.00   1.00   \n",
       "5125   9484180  0.57    0.31   0.34  0.70    0.29   0.35  0.68    0.27   0.33   \n",
       "5126   7932471  0.26    0.26   0.30  0.33    0.34   0.38  0.40    0.40   0.44   \n",
       "5127  71527335  1.00    1.00   1.00  1.00    1.00   1.00  1.00    1.00   1.00   \n",
       "5128  30125442  0.74    0.68   0.67  0.04    0.61   0.85  0.05    0.48   0.69   \n",
       "\n",
       "            color  \n",
       "0           Brown  \n",
       "1            Grey  \n",
       "2           Black  \n",
       "3           Green  \n",
       "4     Multicolour  \n",
       "...           ...  \n",
       "5124        White  \n",
       "5125          Red  \n",
       "5126        Black  \n",
       "5127        White  \n",
       "5128  Multicolour  \n",
       "\n",
       "[5129 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score of 18-nn: 0.59\n",
      "Test set score of 18-nn: 0.57\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "def main(container_path, neigbours = 18, limit = 0):\n",
    "    image_dir = Path(container_path)\n",
    "    count = 0\n",
    "    img_hash = {}\n",
    "    uniq_img = []\n",
    "    \n",
    "    # first of all - correct answers\n",
    "    correct_answers_DF = getCorrectAnswersDF();\n",
    "    \n",
    "    for file in image_dir.iterdir():\n",
    "\n",
    "        img = plt.imread(file)\n",
    "        \n",
    "        img_resized = resize_img(img)\n",
    "        \n",
    "        count+=1;\n",
    "        if limit > 0 and count > limit: break;\n",
    "        \n",
    "        img_arr = img_resized.tolist()\n",
    "        if img_arr in uniq_img: continue;\n",
    "        \n",
    "        pic_id = file.stem\n",
    "        \n",
    "        uniq_img.append(img_arr)\n",
    "        \n",
    "        img_normalized = normalize_image(img_resized)\n",
    "        \n",
    "        img_compressed = compress_img(img_normalized)\n",
    "        \n",
    "        img_pixels_colors = getImgDataSorted(img_compressed)\n",
    "\n",
    "        right_answerDF = correct_answers_DF[correct_answers_DF['pic_id'] == int(pic_id)]\n",
    "        \n",
    "        PopulateData(pic_id, img_pixels_colors, right_answerDF)\n",
    "        \n",
    "        #plt.title(file.stem)\n",
    "        #plt.imshow(img_normalized)  \n",
    "    \n",
    "    inputData = pd.DataFrame(data=inputDataMock)\n",
    "    \n",
    "    inputData = inputData.drop(columns = ['red4', 'green4', 'blue4', 'red5', 'green5', 'blue5']) #\n",
    "    \n",
    "    #inputData = pd.unique(inputData)\n",
    "    \n",
    "    color_counts = inputData['color'].value_counts(ascending=True)\n",
    "\n",
    "    for color, count in color_counts.items():\n",
    "        if(count < 2): inputData.drop(inputData[inputData['color'] == color ].index, inplace=True)\n",
    "        else: break; \n",
    "        \n",
    "    display(inputData)\n",
    "    \n",
    "    X = inputData.drop(columns = ['img_id', 'color'])\n",
    "    Y = inputData['color']\n",
    "    \n",
    "    # split the data in training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, stratify=Y, random_state=69)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=neigbours)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(\"Train set score of \"+str(neigbours)+\"-nn: {:.2f}\".format(knn.score(X_train, y_train)))\n",
    "    print(\"Test set score of \"+str(neigbours)+\"-nn: {:.2f}\".format(knn.score(X_test, y_test)))\n",
    "    \n",
    "    #testAccuracy(X_train, y_train, X_test, y_test)\n",
    "            \n",
    "main(\"datasets/1/images\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a037b46",
   "metadata": {},
   "source": [
    "## 2. Flights Departing from NYC\n",
    "\n",
    "__You should be able to do this exercise after Lecture 4.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e32ed8",
   "metadata": {},
   "source": [
    "For this exercise we will be using the famous nycflights13 data which contains the `airlines`, `airports`, `flights`, `planes`, and `weather` datasets. Please see the documentation (`nycflights13.pdf`) for further information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690cb70",
   "metadata": {},
   "source": [
    "**(a)** Load all files as pandas dataframes and display the first 5 rows of each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df2deb",
   "metadata": {},
   "source": [
    "**(b)** Convert all temperature attributes to degree Celsius. We will be using this in what follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51636f8a",
   "metadata": {},
   "source": [
    "**(c)** Using OLS, investigate if flight distance is associated with arrival delay. You should be cautious regarding negative delays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db424a",
   "metadata": {},
   "source": [
    "**(d)** Using OLS, investigate if departure delay is associated with arrival delay. Again,\n",
    "   consider what to do with negative delays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dbe5e9",
   "metadata": {},
   "source": [
    "**(e)** Investigate whether departure delay is associated with weather conditions\n",
    "   at the origin airport. This includes descriptives, plotting, regression modelling,\n",
    "   considering missing values etc. For regression, do OLS, Ridge, Lasso, and Elastic Net.\n",
    "   The analysis should also include seasonality trends as a \"weather condition\". You could,\n",
    "   for instance, plot the daily departure delay with the date (or monthly). What are the\n",
    "   three most important weather conditions when trying to predict departure delays?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f598d147",
   "metadata": {},
   "source": [
    "**(f)** Is the age of the plane associated with delay? Do OLS, Ridge, Lasso, and Elastic Net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33ca7e",
   "metadata": {},
   "source": [
    "**(g)** Do a principal component analysis of the weather at JFK using the following columns:\n",
    "   temp, dewp, humid, wind_dir, wind_speed, precip, visib.\n",
    "   How many principal components should be used to capture the variability in the weather data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43234aad",
   "metadata": {},
   "source": [
    "**(h)** Build regression models (OLS, Ridge, Lasso, and Elastic Net) that associates\n",
    "   an airports lattitude with weather conditions (temp, dewp, humid, wind_dir, wind_speed,\n",
    "   precip, visib). Remove all but the three most significant whether conditions and redo\n",
    "   the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835084f6",
   "metadata": {},
   "source": [
    "**(i)** On a map, plot the airports that have flights to them where the points that represent\n",
    "   airports are relative in size to the average departure delay. You can see an example in \"airports.png\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d904bc39",
   "metadata": {},
   "source": [
    " **(j)** These questions require no code.\n",
    " - Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter or reduce it?\n",
    "\n",
    "- Why would you want to use:\n",
    "        > Ridge Regression instead of plain Linear Regression (i.e. without any regularization)?\n",
    "        > Lasso instead of Ridge Regression?\n",
    "        > Elastic Net instead of Lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c092b",
   "metadata": {},
   "source": [
    "## 3. Clustering of Handwritten Digits\n",
    "\n",
    "__You should be able to do this exercise after Lecture 5.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc371258",
   "metadata": {},
   "source": [
    "This exercise will depart from the famous MNIST dataset, and we are exploring several clustering techniques with it.. This is a \".mat\" file, in order to load this file in an ipynb you have to use loadmat() function from scipy.io. (replace my path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "mnist = loadmat('mnist-original')\n",
    "mnist_data = mnist[\"data\"].T\n",
    "mnist_label = mnist[\"label\"][0]\n",
    "import numpy as np\n",
    "print(\"Number of datapoints: {}\\n\".format(mnist_data.shape[0]))\n",
    "print(\"Number of features: {}\\n\".format(mnist_data.shape[1]))\n",
    "print(\"List of labels: {}\\n\".format(np.unique(mnist_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575f2c4",
   "metadata": {},
   "source": [
    "There are 70,000 images, and each image has 784 features. This is because each image is 28×28 pixels,\n",
    "and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black). Let’s take a peek at one digit from the dataset. All you need to do is grab an instance’s feature vector, reshape it to a 28×28 array, and display it using Matplotlib’s `imshow()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14d40f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "index = 4\n",
    "print(\"Value of datapoint no. {}:\\n{}\\n\".format(index,mnist_data[index]))\n",
    "print(\"As image:\\n\")\n",
    "plt.imshow(mnist_data[index].reshape(28,28),cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb554a7d",
   "metadata": {},
   "source": [
    "**(a)** Perform k-means clustering with k=10 on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4afa3e5",
   "metadata": {},
   "source": [
    "**(b)** Using visualization techniques analogous to what we have done in the Clustering notebook\n",
    "   for the faces data, can you determine the 'nature' of the 10 constructed clusters?\n",
    "   Do the clusters (roughly) coincide with the 10 different actual digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109f251",
   "metadata": {},
   "source": [
    "**(c)** Perform a supervised clustering evaluation using adjusted rand index.\n",
    "   Are the results stable, when you perform several random restarts of k-means?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab3535",
   "metadata": {},
   "source": [
    "**(d)** Now perform hierarchical clustering on the data.\n",
    "   (in order to improve visibility in the constructed dendrograms, you can also use a\n",
    "   much reduced dataset as constructed using sklearn.utils.resample shown below).\n",
    "   Does the visual analysis of the dendrogram indicate a natural number of clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9340bd9",
   "metadata": {},
   "source": [
    "**(e)** Using different cluster distance metrics (ward,single,average, etc.),\n",
    "   what do the clusterings look like that are produced at the level of k=10 clusters?\n",
    "   See the Clustering notebook for the needed Python code, including the fcluster\n",
    "   method to retrieve 'plain' clusterings from the hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_mnist_data,small_mnist_label = skl.utils.resample(mnist.data,mnist.target,n_samples=200,replace='false')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f07c49",
   "metadata": {},
   "source": [
    "**(f)** Do a DBSCAN clustering of the small dataset. Tweak the different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c004c",
   "metadata": {},
   "source": [
    "**(g)** Try to compare the different clustering methods on the MNIST dataset in the same way\n",
    "   the book does on the faces dataset on pp. 195-206."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd3830",
   "metadata": {},
   "source": [
    "## 4. The Local Elections\n",
    "\n",
    "__You should be able to do this exercise after Lecture 6.__\n",
    "\n",
    "In the local elections of 2021, around 100 candidates stood for election for the city council of Horsens. 83 of them represented a national party, had more than one candidate and provided answers to the <a href=\"https://www.dr.dk/nyheder/politik/kandidattest\">DR Candidate Test</a>, a test designed to help voters find out who they should vote for. In this test, the candidates answered 18 questions, which we will use as features in the following. The politicians belong to 9 parties, which will be our classes.\n",
    "\n",
    "The numpy files `X_Horsens.npy` and `Y_Horsens.npy` contains the data. `Y_Horsens.npy` contains a letter representing the party to which each candidate belongs. The following parties are represented:\n",
    "\n",
    "| Party letter | Party name | Party name (English) | Political position | Party color |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| A | Socialdemokratiet | Social Democrats | Centre-left | Red |\n",
    "| B | Radikale Venstre | Social Liberal Party | Centre-left | Indigo |\n",
    "| C | Det Konservative Folkeparti | Conservative People's Party | Right-wing | Green |\n",
    "| D | Nye Borgerlige | New Right | Far-right | Black |\n",
    "| F | Socialistisk Folkeparti | Socialist People's Party | Left-wing | Fuchsia |\n",
    "| I | Liberal Alliance | Liberal Alliance | Right-wing | Cyan |\n",
    "| O | Dansk Folkeparti | Danish People's Party | Far-right | Yellow |\n",
    "| V | Venstre | Danish Liberal Party | Centre-right | Blue |\n",
    "| Z* | Enhedslisten | Red-Green Alliance | Far-left | Dark red |\n",
    "\n",
    "*_Note that, although the party letter of Enhedslisten is actually Ø, we will here use Z to avoid any complications with the wonderful Danish letters Æ, Ø and Å. Feel free to change the Z back to an Ø if you find that it does not cause any problems._\n",
    "\n",
    "Meanwhile, `X_Horsens.npy` contains the answers to the test as numbers between -1.5 and 1.5, such that -1.5 is \"Strongly disagree\", -0.5 is \"Disagree\", 0.5 is \"Agree\" and 1.5 is \"Strongly agree\". The 18 questions concern, in order, subdivision, schools, windmills, building permits, tall buildings, housing, child care, culture, nursing homes, taxes, sports, refugees, nursing homes (again), public transportation, meat-free days, welfare, privatization, and religious minorities.\n",
    "\n",
    "Both files can be imported using `numpy.load`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928c2d6",
   "metadata": {},
   "source": [
    "__(a)__ How well do you (intuitively) expect that we can predict the partisan affiliation of a candidate based on their answers to the test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfefd89d",
   "metadata": {},
   "source": [
    "__(b)__ Based on the answers from all 83 candidates for the Horsens city council, perform a Principal Component Analysis with 2 principal components. Plot the results in a figure using these 2 components as the axes. Label the points with the party letter and the appropriate color."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd744fdf",
   "metadata": {},
   "source": [
    "__(c)__ Comment on the results. You may consider the following questions for inspiration: Can the political parties be separated? Can the typical distinction of \"left-wing\" and \"right-wing\" be discerned? Which of the 18 questions (features) are most important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19658e1",
   "metadata": {},
   "source": [
    "The number of candidates (83) is on the (very) low side when we want to do machine learning. Luckily, the neighbouring city of Databorg had no less than 8,300 candidates standing for election, with a political environment similar to that of Horsens. In the following, we will use the data from Databorg. These are stored in the numpy files `X_Databorg.npy` and `Y_Databorg.npy` in same format as the Horsens data.\n",
    "\n",
    "__(d)__ Once again, perform a Principal Component Analysis and visualize the results. Compare the results to those of the Horsens data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63a135",
   "metadata": {},
   "source": [
    "Confident that we can predict the partisan affiliation of a politician reasonably well based on their answers to the test, we want to build a model that will allow us to distinguish between the 9 political parties. For this purpose, we split the data into a training and a validation set.\n",
    "\n",
    "__(e)__ Split the data into a training and a validation set, with appropriate fractions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc5200",
   "metadata": {},
   "source": [
    "First, we assume that a Naive Bayes approach is sufficient for our purposes.\n",
    "\n",
    "__(f)__ Comment on the basic assumption of the Naive Bayes approach. Is this a reasonable assumption for the problem at hand?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153bbd8",
   "metadata": {},
   "source": [
    "__(g)__ Classify the instances of the validation set using a Naive Bayes approach. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71c63d",
   "metadata": {},
   "source": [
    "Assume instead that a _k_-nearest neighbour approach is sufficient for our  needs.\n",
    "\n",
    "__(h)__ Using default settings of the _k_-NN classifier, classify the instances of the validation set. Comment on the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a201e2",
   "metadata": {},
   "source": [
    "__(i)__ Play around with different values of _k_. Decide on a \"good\" value of _k_. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1735c72",
   "metadata": {},
   "source": [
    "We now try to use a decision tree instead.\n",
    "\n",
    "__(j)__ What is the _minimum_ depth of an appropriate decision tree? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1438f0",
   "metadata": {},
   "source": [
    "__(k)__ Build a decision tree with at least the depth from above. Play around with the tree depth. Include a figure that shows some relevant measure of the performance as a function of the tree depth. Comment on any issues of over-fitting. Decide on a tree which you will keep for later use. Can you do better than the _k_-NN classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f031f805",
   "metadata": {},
   "source": [
    "__(l)__ What are the most important features? Visualize this in an appropriate way. Does it match what you would expect? Compare to the results of the PCA analysis. Do we expect them to be the same? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c3ce0",
   "metadata": {},
   "source": [
    "We know that decision trees suffer from certain problems that may be solved by using decision forests.\n",
    "\n",
    "__(m)__ Build a decision forest. Play around with the number of trees in the forest. Decide on a forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da7963f",
   "metadata": {},
   "source": [
    "__(n)__ Extract the most important features. Comment and compare with previously obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc53cac",
   "metadata": {},
   "source": [
    "Finally, we want to compare the models we have worked with so far (i.e., Naive Bayes, _k_-NN, decision tree and decision forest).\n",
    "\n",
    "__(o)__ Compare the results of the in terms of confusion matrices, accuracy, precision, recall, and f-score. How well can we predict the partisan affiliation of a candidate based on their answers to a test? How does this compare with your intuition? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d1258",
   "metadata": {},
   "source": [
    "## 5. Sentiment Analysis\n",
    "\n",
    "__You should be able to do this exercise after Lecture 8.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f62c6",
   "metadata": {},
   "source": [
    "In this exercise we use the IMDb-dataset, which we will use to perform a sentiment analysis. The code below assumes that the data is placed in the same folder as this notebook. We see that the reviews are loaded as a pandas dataframe, and print the beginning of the first few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929cfecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('reviews.txt', header=None)\n",
    "labels = pd.read_csv('labels.txt', header=None)\n",
    "Y = (labels=='positive').astype(np.int_)\n",
    "\n",
    "print(type(reviews))\n",
    "print(reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f02cccd",
   "metadata": {},
   "source": [
    "**(a)** Split the reviews and labels in test, train and validation sets. The train and validation sets will be used to train your model and tune hyperparameters, the test set will be saved for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af469bc",
   "metadata": {},
   "source": [
    "**(b)** Use the `CountVectorizer` from `sklearn.feature_extraction.text` to create a Bag-of-Words representation of the reviews. (See an example of how to do this in chapter 7 of \"Muller and Guido\"). Only use the 10,000 most frequent words (use the `max_features`-parameter of `CountVectorizer`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd37af",
   "metadata": {},
   "source": [
    "**(c)** Explore the representation of the reviews. How is a single word represented? How about a whole review?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfdabb7",
   "metadata": {},
   "source": [
    "**(d)** Train a neural network with a single hidden layer on the dataset, tuning the relevant hyperparameters to optimize accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d55d2",
   "metadata": {},
   "source": [
    "**(e)** Test your sentiment-classifier on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae2b46",
   "metadata": {},
   "source": [
    "**(h)** Use the classifier to classify a few sentences you write yourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63bd08",
   "metadata": {},
   "source": [
    "## 6. Speech Recognition\n",
    "\n",
    "__You should be able to do this exercise after Lecture 9.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d260e",
   "metadata": {},
   "source": [
    "In this exercise, we will work with the <a href=\"https://arxiv.org/pdf/1804.03209.pdf\">Google Speech Command Dataset</a>, which can be downloaded from <a href=\"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\">here</a> (note: you do not need to download the full dataset, but it will allow you to play around with the raw audiofiles). This dataset contains 105,829 one-second long audio files with utterances of 35 common words.\n",
    "\n",
    "We will use a subset of this dataset as indicated in the table below.\n",
    "\n",
    "| Word | How many? | Class # |\n",
    "| :-: | :-: | :-: |\n",
    "| Yes | 4,044 | 3 |\n",
    "| No | 3,941 | 1 |\n",
    "| Stop | 3,872 | 2 |\n",
    "| Go | 3,880 | 0 |\n",
    "\n",
    "The data is given in the files `XSound.npy` and `YSound.npy`, both of which can be imported using `numpy.load`. `XSound.npy` contains spectrograms (_e.g._, matrices with a time-axis and a frequency-axis of size 62 (time) x 65 (frequency)). `YSound.npy` contains the class number, as indicated in the table above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4736285",
   "metadata": {},
   "source": [
    "__(a)__ Explore and prepare the data, including splitting the data in training, validation and testing data, handling outliers, perhaps taking logarithms, etc. Data preparation is - as always - quite important. Document what you do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed2ab5",
   "metadata": {},
   "source": [
    "__(b)__ Visualize a few examples of yes's, no's, stop's and go's, so that you have a reasonable intuitive understanding of the difference between the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceac378",
   "metadata": {},
   "source": [
    "__(c)__ Train a neural network and at least one other algorithm on the data. Find a good set of hyperparameters for each model. Do you think a neural network is suitable for this kind of problem? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c4f8f",
   "metadata": {},
   "source": [
    "__(d)__ Classify instances of the validation set using your models. Comment on the results in terms of metrics you have learned in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5689b4",
   "metadata": {},
   "source": [
    "__(e)__ Identify (a few) misclassified words, including what they are misclassified as. Visualize them as before, and compare with your intuitive understanding of how the words look. Do you find the misclassified examples surprising?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f5e76",
   "metadata": {},
   "source": [
    "## 7. Group Assignment & Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db549c55",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__You should be able to start up on this exercise after Lecture 1.__\n",
    "\n",
    "*This exercise must be a group effort. That means everyone must participate in the assignment.*\n",
    "\n",
    "In this assignment you will solve a data science problem end-to-end, pretending to be recently hired data scientists in a company. To help you get started, we've prepared a checklist to guide you through the project. Here are the main steps that you will go through:\n",
    "\n",
    "1. Frame the problem and look at the big picture\n",
    "2. Get the data\n",
    "3. Explore and visualise the data to gain insights\n",
    "4. Prepare the data to better expose the underlying data patterns to machine learning algorithms\n",
    "5. Explore many different models and short-list the best ones\n",
    "6. Fine-tune your models\n",
    "7. Present your solution \n",
    "\n",
    "In each step we list a set of questions that one should have in mind when undertaking a data science project. The list is not meant to be exhaustive, but does contain a selection of the most important questions to ask. We will be available to provide assistance with each of the steps, and will allocate some part of each lesson towards working on the projects.\n",
    "\n",
    "Your group must submit a _**single**_ Jupyter notebook, structured in terms of the first 6 sections listed above (the seventh will be a video uploaded to some streaming platform, e.g. YouTube, Vimeo, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85806b",
   "metadata": {},
   "source": [
    "### 1. Analysis: Frame the problem and look at the big picture\n",
    "1. Find a problem/task that everyone in the group finds interesting\n",
    "2. Define the objective in business terms\n",
    "3. How should you frame the problem (supervised/unsupervised etc.)?\n",
    "4. How should performance be measured?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf807c0",
   "metadata": {},
   "source": [
    "### 2. Get the data\n",
    "1. Find and document where you can get the data from\n",
    "2. Get the data\n",
    "3. Check the size and type of data (time series, geographical etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289787da",
   "metadata": {},
   "source": [
    "### 3. Explore the data\n",
    "1. Create a copy of the data for explorations (sampling it down to a manageable size if necessary)\n",
    "2. Create a Jupyter notebook to keep a record of your data exploration\n",
    "3. Study each feature and its characteristics:\n",
    "    * Name\n",
    "    * Type (categorical, int/float, bounded/unbounded, text, structured, etc)\n",
    "    * Percentage of missing values\n",
    "    * Check for outliers, rounding errors etc\n",
    "4. For supervised learning tasks, identify the target(s)\n",
    "5. Visualise the data\n",
    "6. Study the correlations between features\n",
    "7. Identify the promising transformations you may want to apply (e.g. convert skewed targets to normal via a log transformation)\n",
    "8. Document what you have learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6442d914",
   "metadata": {},
   "source": [
    "### 4. Prepare the data\n",
    "Notes:\n",
    "* Work on copies of the data (keep the original dataset intact).\n",
    "* Write functions for all data transformations you apply, for three reasons:\n",
    "    * So you can easily prepare the data the next time you run your code\n",
    "    * So you can apply these transformations in future projects\n",
    "    * To clean and prepare the test set\n",
    "    \n",
    "    \n",
    "1. Data cleaning:\n",
    "    * Fix or remove outliers (or keep them)\n",
    "    * Fill in missing values (e.g. with zero, mean, median, regression ...) or drop their rows (or columns)\n",
    "2. Feature selection (optional):\n",
    "    * Drop the features that provide no useful information for the task (e.g. a customer ID is usually useless for modelling).\n",
    "3. Feature engineering, where appropriate:\n",
    "    * Discretize continuous features\n",
    "    * Use one-hot encoding if/when relevant\n",
    "    * Add promising transformations of features (e.g. $\\log(x)$, $\\sqrt{x}$, $x^2$, etc)\n",
    "    * Aggregate features into promising new features\n",
    "4. Feature scaling: standardise or normalise features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e836f5e",
   "metadata": {},
   "source": [
    "### 5. Short-list promising models\n",
    "We expect you to do some additional research and train at **least one model per team member**.\n",
    "\n",
    "1. Train mainly quick and dirty models from different categories (e.g. linear, SVM, Random Forests etc) using default parameters\n",
    "2. Measure and compare their performance\n",
    "3. Analyse the most significant variables for each algorithm\n",
    "4. Analyse the types of errors the models make\n",
    "5. Have a quick round of feature selection and engineering if necessary\n",
    "6. Have one or two more quick iterations of the five previous steps\n",
    "7. Short-list the top three to five most promising models, preferring models that make different types of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fdb683",
   "metadata": {},
   "source": [
    "### 6. Fine-tune the system\n",
    "1. Fine-tune the hyperparameters\n",
    "2. Once you are confident about your final model, measure its performance on the test set to estimate the generalisation error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c46de",
   "metadata": {},
   "source": [
    "### 7. Present your solution\n",
    "1. Document what you have done\n",
    "2. Create a nice 15 minute video presentation with slides\n",
    "    * Make sure you highlight the big picture first\n",
    "3. Explain why your solution achieves the business objective\n",
    "4. Don't forget to present interesting points you noticed along the way:\n",
    "    * Describe what worked and what did not\n",
    "    * List your assumptions and you model's limitations\n",
    "5. Ensure your key findings are communicated through nice visualisations or easy-to-remember statements (e.g. \"the median income is the number-one predictor of housing prices\")\n",
    "6. Upload the presentation to some online platform, e.g. YouTube or Vimeo, and supply a link to the video in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725eef98",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a382ea",
   "metadata": {},
   "source": [
    "Géron, A. 2017, *Hands-On Machine Learning with Scikit-Learn and Tensorflow*, Appendix B, O'Reilly Media, Inc., Sebastopol."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
