{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc732fc",
   "metadata": {},
   "source": [
    "# Machine Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Moskalchuk 293172\n",
    "# Julia Tankiewicz 293719\n",
    "# Nikita Bondarchuk 294478"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edc094",
   "metadata": {},
   "source": [
    "The assignments below should be solved and documented as a project that will form the basis for the\n",
    "examination. When solving the exercises it is important that you\n",
    "\n",
    "  * document all relevant results and analyses that you have obtained/performed during the exercises.\n",
    "  * try to relate your results to the theoretical background of the methods being applied.\n",
    "\n",
    "Feel free to add cells if you need to.\n",
    "\n",
    "Please hand in assignment 1-6 in a _**single**_ Jupyter notebook where you retain the questions outlined below. You are welcome to adapt code from the web (e.g. Kaggle kernels), but you **_must_** reference the original source in your notebook. In addition to _clean, well-documented code_ (i.e. functions with <a href=\"https://www.geeksforgeeks.org/python-docstrings/\">docstrings</a>, etc), your notebook will be judged according to how well each step is explained (using Markdown). \n",
    "\n",
    "In general, direct questions regarding assignments 1, 4, 5 and 6 to Frederik, and questions regarding assignments 2, 3, and 7 to Richard. \n",
    "\n",
    "Last, but not least:\n",
    "* Looking for an overview of the markdown language? The cheat sheet <a href=\"https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\">here</a> might help.\n",
    "* For the Python specific components of the exercises, you should not need constructs beyond those that are already included in the notebooks on the course's web-page (still you should not feel constrained by these, so feel free to be adventurous). You may, however, need to consult the documentation for some of the methods supplied by `sklearn`.\n",
    "\n",
    "**Groups:** Create your own groups. May be across teams. 2-4 students per group. No one-person groups.\n",
    "\n",
    "\n",
    "**Submission deadline:** Thursday, December 15 before 13.00 CET (Notebooks + presentation recording)\n",
    "\n",
    "**Expected workload:** Each student is expected to spend around around 50 hours on the project.\n",
    "\n",
    "### Deliverables\n",
    "The teams have to submit three deliverables before the submission deadline: 1) a notebook of assignments 1-6, 2) a notebook of assignment 7, and 3) presentation video uploaded to some online platform e.g. YouTube, Vimeo, etc.\n",
    "\n",
    "#### Notebook\n",
    "The notebook contains all the code to explore the dataset, train the final model and documents each step clearly. If code is copied from another codebase such as Github or Stack Overflow it **_must_** be properly referenced.\n",
    "\n",
    "\n",
    "#### Presentation\n",
    "The presentation video should be 15 min long and should highlight the problem you are solving, interesting things you found in the data and the step involved in building up your model. At the exam we will discuss the presentation and ask questions about your project and submissions. A link to the video must be placed in the notebook for assignment 7.\n",
    "\n",
    "### Randomness\n",
    "For ALL random states, choose state = 69 so we can replicate your work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary modules here:\n",
    "#To use following packages, pip install them first \n",
    "#pip install opencv-python\n",
    "#pip install keras\n",
    "#pip install --no-cache-dir tensorflow\n",
    "#pip install shapely\n",
    "#pip install geopandas\n",
    "#pip install graphviz\n",
    "#pip install pydotplus\n",
    "#pip install tensorflow\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import ast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler \n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "from sklearn import (decomposition, ensemble, metrics)\n",
    "\n",
    "\n",
    "#from sklearn import (neighbors, datasets, decomposition, ensemble,metrics, model_selection, preprocessing, linear_model)\n",
    "#from keras.preprocessing import image\n",
    "#from tensorflow.keras.utils import load_img, img_to_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c9417",
   "metadata": {},
   "source": [
    "## 1. The IceCat Dataset\n",
    "\n",
    "__You should be able to do this exercise after Lecture 3.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7bcb69",
   "metadata": {},
   "source": [
    "The IceCat Dataset, kindly provided to us by Stibo Systems, contains a large amount of data on different office products. As an example of \"real-world\" data, these data are imperfect and incomplete. As such, this exercise is not so much an exercise in creating a good machine learning model, but places a larger emphasis on \"cleaning the data\".\n",
    "\n",
    "We are going to work with a subset of the IceCat Dataset. In particular, you will be provided with a zip file of 5,854 images of office products, each with the name \"product ID\".jpg. You will also be provided with a list of colors, `colors.txt`, which, when imported using the code below, is a list of tuples of the form `[(\"product ID\", \"color\"), ...]`. (The code below assumes that `colors.txt` is in the same folder as the jupyter notebook. Feel free to change the code if you prefer a different organization of your files).\n",
    "\n",
    "Your task is to clean up the data and construct a simple machine learning model (_e.g._, _k_-nearest neighbor) that can identify the color of a product. You have free hands - there is hardly any one \"correct answer\" - but you need to argue for your choices. Among other things, you probably need to think about the following as you work with the data:\n",
    "\n",
    "* All of the images have different sizes.\n",
    "\n",
    "* Some of the images are RGB images (3 layers), others are CMYK (4 layers), some might even be black-and-white (1 layer).\n",
    "\n",
    "* Some colors are only represented by very few products.\n",
    "\n",
    "* Some colors are very similar, such as \"Purple\" and \"Violet\".\n",
    "\n",
    "* A product may have a particular color, but a packaging of a different color. Similarly, the color of, say, a computer monitor may be black, while the image of it could show a monitor that is turned on with a green screensaver.\n",
    "\n",
    "* Many products are attributed to several colors, such as \"Black, Blue\" or even \"Blue, Green, Orange, Violet, Yellow\". Yet others are described as \"Multicolor\" or \"Assorted colors\".\n",
    "\n",
    "Again, you have free hands in how you are going to solve these (and other) challenges, but you must argue for and reflect on your choices as you progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mappings = {\n",
    "  # source color => arr of colors associated with the source color\n",
    "    'Grey' : ['Silver', 'Metallic', 'Aluminium', 'Stainless steel', 'Platinum', 'Graphite', 'Chrome', 'Anthracite','Light Grey','Charcoal','Titanium','Light grey'],\n",
    "    'Purple' : ['Violet', 'Lavender', 'Fuchsia','Magenta'],\n",
    "    'Transparent' : ['Translucent'],\n",
    "    'Cream' : ['Beige', 'Sand', 'Tan','Cappuccino','Ivory'],\n",
    "    'Brown' : ['Oak colour', 'Wood','Bronze'],\n",
    "    'Cyan' : ['Turquoise', 'Aqua colour'],\n",
    "    'White' : ['Pearl' ],\n",
    "    'Pink' : ['Rose'],\n",
    "    'Red' : ['Cherry', 'Bordeaux'],\n",
    "    'Green' : ['Pine', 'Lime','Olive'],\n",
    "    'Yellow' : ['Gold'],\n",
    "    'Blue' : ['Navy'],\n",
    "    'Multicolour' : ['Multi','Assorted colours']\n",
    "}\n",
    "\n",
    "inputDataMock = {\n",
    "    'img_id': [],\n",
    "    'red1': [], 'green1': [], 'blue1': [],\n",
    "    'red2': [], 'green2': [], 'blue2': [],\n",
    "    'red3': [], 'green3': [], 'blue3': [],\n",
    "    'red4': [], 'green4': [], 'blue4': [],\n",
    "    'red5': [], 'green5': [], 'blue5': [],\n",
    "    'color': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e29af8",
   "metadata": {},
   "source": [
    "___---METHODS---___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e202dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the source color in response to the associated color using the color_mappings\n",
    "def colors_binning(initial_color):\n",
    "    for source_color in color_mappings:\n",
    "        if initial_color in color_mappings[source_color]: return source_color;\n",
    "    return initial_color;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceab872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dataframe from colors.txt\n",
    "def getCorrectAnswersDF():\n",
    "    with open(\"datasets/1/colors.txt\",\"r\") as file:\n",
    "        colors = ast.literal_eval(file.read())\n",
    "        # lets create a dataframe\n",
    "        d = {'pic_id': [], 'color': []}\n",
    "        for row in colors:\n",
    "            # check if there are more colors in one row\n",
    "            colors_arr = row[1].split(\", \")\n",
    "            d['pic_id'].append(row[0])\n",
    "            d['color'].append(colors_binning(colors_arr[0])) \n",
    "\n",
    "        pic_colors = pd.DataFrame(data=d)\n",
    "        return pic_colors;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e140c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImgDataSorted(imgData, ignoreWhite = True, colorRound = 2, whiteThreshold = 0.15):\n",
    "    pixel_arr = {}\n",
    "    white = 1 - whiteThreshold;\n",
    "    # itearate over images rows\n",
    "    for imgRow in imgData:\n",
    "        # iterate over pixels\n",
    "        for pixelData in imgRow:\n",
    "            red = round(pixelData.item(0) / 255, colorRound)\n",
    "            green = round(pixelData.item(1) / 255, colorRound)\n",
    "            blue = round(pixelData.item(2) / 255, colorRound)\n",
    "            \n",
    "            # skip white pixels\n",
    "            if (ignoreWhite and red >= white and green >= white and blue >= white): \n",
    "                continue;\n",
    "                \n",
    "            key = str(red) +' '+ str(green) +' '+ str(blue);\n",
    "            # if this pixel color is found in pixel_arr\n",
    "            if (pixel_arr.get(key)): \n",
    "                pixel_arr[key]['count'] += 1 # add 1 to the counter\n",
    "            else: # insert the color, set the counter to 1\n",
    "                pixel_arr[key] = {'R': red, 'G': green, 'B': blue, 'count': 1};\n",
    "    # sort the pixels by number of occurencies and return\n",
    "    return sorted(pixel_arr.values(), key=lambda x: x['count'], reverse=True)\n",
    "\n",
    "def PopulateData(pic_id, pixel_arr, correctAnswer):      \n",
    "    # move the data to the dataFrame mock\n",
    "    inputDataMock['img_id'].append(pic_id)\n",
    "    \n",
    "    for i in range(1,6):\n",
    "        try:\n",
    "            color = pixel_arr[i-1]\n",
    "        except IndexError: # Index out of range - no more colors found\n",
    "            color = {'R': 1.0, 'G': 1.0, 'B': 1.0}\n",
    "            \n",
    "        inputDataMock['red'+str(i)].append(color['R'])\n",
    "        inputDataMock['green'+str(i)].append(color['G'])\n",
    "        inputDataMock['blue'+str(i)].append(color['B'])\n",
    "    \n",
    "    for row_id, row in correctAnswer.iterrows():\n",
    "        col = str(row['color']).strip()\n",
    "        if len(col) == 0: col = None;\n",
    "        \n",
    "        inputDataMock['color'].append(col)\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0514454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAccuracy(X_train, y_train, X_test, y_test):#lets see how accuracy depends on number of neighbours\n",
    "    #training_accuracy = []\n",
    "    #test_accuracy = []\n",
    "    diff_arr = []\n",
    "    # try n_neighbors from 1 to 10\n",
    "    neighbors_settings = range(3, 25)\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # build the model\n",
    "        clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        clf.fit(X_train, y_train)\n",
    "        # record training set accuracy\n",
    "        train = clf.score(X_train, y_train)\n",
    "        test = clf.score(X_test, y_test)\n",
    "        diff = (train - test) / ((train + test) * 0.5)\n",
    "        #training_accuracy.append(train)\n",
    "        # record generalization accuracy\n",
    "        #test_accuracy.append(test)\n",
    "        diff_arr.append(diff)\n",
    "\n",
    "    #plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "    #plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "    plt.plot(neighbors_settings, diff_arr, label=\"diff\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"n_neighbors\")\n",
    "    plt.legend(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19bc922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img, dimension=(64, 64)):\n",
    "    img_resized = cv.resize(img, dimension, interpolation=cv.INTER_AREA)\n",
    "    return img_resized\n",
    "\n",
    "def normalize_img(img):\n",
    "    if img.shape == (64, 64):\n",
    "        img = cv.cvtColor(img.astype('float32'),cv.COLOR_GRAY2RGB)\n",
    "    elif img.shape == (64, 64, 4):\n",
    "        img = cv.cvtColor(img.astype('float32'),cv.COLOR_RGBA2RGB)\n",
    "\n",
    "    return img\n",
    "\n",
    "def compress_img(img, nr_of_colors = 16):\n",
    "    \n",
    "    # Lets compress the image using clustering\n",
    "    # https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_ml/py_kmeans/py_kmeans_opencv/py_kmeans_opencv.html\n",
    "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "    flags = cv.KMEANS_PP_CENTERS\n",
    "    compactness, labels, centers = cv.kmeans(np.float32(img.reshape((-1, 3))),\n",
    "                                    nr_of_colors, None, criteria, 10, flags)\n",
    "    centers = np.uint8(centers)\n",
    "    new_colors = centers[labels.flatten()]\n",
    "    img_noramlized = new_colors.reshape(img.shape)\n",
    "    return img_noramlized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "def main(container_path, neigbours = 18, limit = 0):\n",
    "    image_dir = Path(container_path)\n",
    "    count = 0\n",
    "    img_hash = {}\n",
    "    uniq_img = []\n",
    "    \n",
    "    # first of all - correct answers\n",
    "    correct_answers_DF = getCorrectAnswersDF();\n",
    "    \n",
    "    for file in image_dir.iterdir():\n",
    "\n",
    "        img = plt.imread(file)\n",
    "        \n",
    "        img_resized = resize_img(img)\n",
    "        \n",
    "        count+=1;\n",
    "        if limit > 0 and count > limit: break;\n",
    "        \n",
    "        img_arr = img_resized.tolist()\n",
    "        if img_arr in uniq_img: continue;\n",
    "        \n",
    "        pic_id = file.stem\n",
    "        \n",
    "        uniq_img.append(img_arr)\n",
    "        \n",
    "        img_normalized = normalize_img(img_resized)\n",
    "        \n",
    "        img_compressed = compress_img(img_normalized)\n",
    "        \n",
    "        img_pixels_colors = getImgDataSorted(img_compressed)\n",
    "\n",
    "        right_answerDF = correct_answers_DF[correct_answers_DF['pic_id'] == int(pic_id)]\n",
    "        \n",
    "        PopulateData(pic_id, img_pixels_colors, right_answerDF)\n",
    "        \n",
    "        #plt.title(file.stem)\n",
    "        #plt.imshow(img_normalized)  \n",
    "    \n",
    "    inputData = pd.DataFrame(data=inputDataMock)\n",
    "    \n",
    "    inputData = inputData.drop(columns = ['red4', 'green4', 'blue4', 'red5', 'green5', 'blue5']) #\n",
    "    \n",
    "    #inputData = pd.unique(inputData)\n",
    "    \n",
    "    color_counts = inputData['color'].value_counts(ascending=True)\n",
    "\n",
    "    for color, count in color_counts.items():\n",
    "        if(count < 2): inputData.drop(inputData[inputData['color'] == color ].index, inplace=True)\n",
    "        else: break; \n",
    "        \n",
    "    display(inputData)\n",
    "    \n",
    "    X = inputData.drop(columns = ['img_id', 'color'])\n",
    "    Y = inputData['color']\n",
    "    \n",
    "    # split the data in training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, stratify=Y, random_state=69)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=neigbours)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(\"Train set score of \"+str(neigbours)+\"-nn: {:.2f}\".format(knn.score(X_train, y_train)))\n",
    "    print(\"Test set score of \"+str(neigbours)+\"-nn: {:.2f}\".format(knn.score(X_test, y_test)))\n",
    "    \n",
    "    #testAccuracy(X_train, y_train, X_test, y_test)\n",
    "            \n",
    "main(\"datasets/1/images\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a037b46",
   "metadata": {},
   "source": [
    "## 2. Flights Departing from NYC\n",
    "\n",
    "__You should be able to do this exercise after Lecture 4.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e32ed8",
   "metadata": {},
   "source": [
    "For this exercise we will be using the famous nycflights13 data which contains the `airlines`, `airports`, `flights`, `planes`, and `weather` datasets. Please see the documentation (`nycflights13.pdf`) for further information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690cb70",
   "metadata": {},
   "source": [
    "**(a)** Load all files as pandas dataframes and display the first 5 rows of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JULIA\n",
    "#loading\n",
    "airlines = pd.read_csv('datasets/2/airlines.csv')\n",
    "airports = pd.read_csv('datasets/2/airports.csv')\n",
    "flights = pd.read_csv('datasets/2/flights.csv')\n",
    "planes = pd.read_csv('datasets/2/planes.csv')\n",
    "weather = pd.read_csv('datasets/2/weather.csv')\n",
    "#displaying\n",
    "display(airlines.head())\n",
    "display(airports.head())\n",
    "display(flights.head())\n",
    "display(planes.head())\n",
    "display(weather.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df2deb",
   "metadata": {},
   "source": [
    "**(b)** Convert all temperature attributes to degree Celsius. We will be using this in what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JULIA\n",
    "weather['temp'] = (weather['temp']-32)*5/9\n",
    "weather['temp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51636f8a",
   "metadata": {},
   "source": [
    "**(c)** Using OLS, investigate if flight distance is associated with arrival delay. You should be cautious regarding negative delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fbd4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JULIA\n",
    "\n",
    "positive_delays = flights[flights['arr_delay']>0]\n",
    "negative_delays = flights[flights['arr_delay']<0]\n",
    "\n",
    "def plotRegression(feature,df):\n",
    "    df = df.dropna(subset=[feature,'arr_delay'])\n",
    "    X = df[[feature]]\n",
    "    y = df['arr_delay']\n",
    "    X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "    ODS = LinearRegression()\n",
    "    ODS.fit(X_train, y_train)\n",
    "\n",
    "    print(\"R^2 on train data is {} and on test data is {}\".format(ODS.score(X_train, y_train),ODS.score(X_test,y_test)))\n",
    "    print(\"The coefficents are {} and the intercept is {}\".format(ODS.coef_, ODS.intercept_))\n",
    "    print(df[['arr_delay',feature]].corr())\n",
    "    \n",
    "    y_pred = ODS.predict(X)\n",
    "    plt.scatter(X, y,color = \"green\") \n",
    "    plt.plot(X[feature], y_pred,color = \"blue\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Delay\")\n",
    "    plt.show()\n",
    "\n",
    "plotRegression('distance',flights) #All fligths \n",
    "plotRegression('distance',positive_delays)#flights with only positive delays\n",
    "plotRegression('distance',negative_delays)#flights with only negative delays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a443490",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "According to graphs above the arrival delay and distance are correlated almost only in case of negative delays. That means that flights flying further distances were deleyed less(so they arrived sooner) than flights flying short distances, but that's true mostly only for when the flights departed ealier than scheduled(3rd graph). For flights with regular(positive) delays there's hardly any correlation between delay time and distance(2nd graph). Concluding, there's very little correlation between two features and it's mostly influenced by negative delays. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db424a",
   "metadata": {},
   "source": [
    "**(d)** Using OLS, investigate if departure delay is associated with arrival delay. Again,\n",
    "   consider what to do with negative delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c14bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JULIA\n",
    "\n",
    "positive_delays = flights[((flights['dep_delay']>0) & (flights['arr_delay']>0))]\n",
    "negative_delays = flights[((flights['dep_delay']<0) & (flights['arr_delay']<0))]\n",
    "\n",
    "def plotRegression(feature,df):\n",
    "    df = df.dropna(subset=[feature,'arr_delay'])\n",
    "    X = df[[feature]]\n",
    "    y = df['arr_delay']\n",
    "    X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "    ODS = LinearRegression()\n",
    "    ODS.fit(X_train, y_train)\n",
    "\n",
    "    print(\"R^2 on train data is {} and on test data is {}\".format(ODS.score(X_train, y_train),ODS.score(X_test,y_test)))\n",
    "    print(\"The coefficents are {} and the intercept is {}\".format(ODS.coef_, ODS.intercept_))\n",
    "    print(df[['arr_delay',feature]].corr())\n",
    "    \n",
    "    y_pred = ODS.predict(X)\n",
    "    plt.scatter(X, y,color = \"green\") \n",
    "    plt.plot(X[feature], y_pred,color = \"blue\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Delay\")\n",
    "    plt.show()\n",
    "\n",
    "plotRegression('dep_delay',flights) #All fligths \n",
    "plotRegression('dep_delay',positive_delays)#flights with only positive delays\n",
    "plotRegression('dep_delay',negative_delays)#flights with only negative delays\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0609662a",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "The correlation between departure and arrival time is big in case of late flights but little in case of fligts that departed before expected time. So the correlations between these two is high but lowered by negative delays where arrival delay doesn't seem to be influenced by how early(earlier than expected time) the flight departed. It could be explained by fact that if flight departed ealier than expected it won't rush and usually land in planned time as it's probably scheduled and airports might not often allow early arrivals(it's just hypothesis :)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dbe5e9",
   "metadata": {},
   "source": [
    "**(e)** Investigate whether departure delay is associated with weather conditions\n",
    "   at the origin airport. This includes descriptives, plotting, regression modelling,\n",
    "   considering missing values etc. For regression, do OLS, Ridge, Lasso, and Elastic Net.\n",
    "   The analysis should also include seasonality trends as a \"weather condition\". You could,\n",
    "   for instance, plot the daily departure delay with the date (or monthly). What are the\n",
    "   three most important weather conditions when trying to predict departure delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JULIA\n",
    "display(flights.head())\n",
    "display(weather.head())\n",
    "#remove flights with no hour data - unable to be merged with weather data\n",
    "flights_complete = flights.dropna(subset=['hour'])\n",
    "#remove unrelated features\n",
    "flights_complete = flights_complete.drop(columns=['carrier', 'tailnum','flight','dest','air_time','distance','arr_delay','arr_time','minute'])\n",
    "flights_complete.info()\n",
    "weather.info()\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging weather and flights data and leaving only ones with departure delay non-nan values\n",
    "flight_weather = pd.merge(flights_complete, weather, how='outer', on=['origin', 'year','month','day','hour'])\n",
    "flight_weather = flight_weather.drop(columns=['origin','time_hour','year'])\n",
    "flight_weather = flight_weather.dropna(subset=['dep_delay'])\n",
    "\n",
    "\n",
    "#flight_weather[feature] = scaler.fit_transform(flight_weather[[feature]])\n",
    "\n",
    "flight_weather.head()\n",
    "flight_weather.info()\n",
    "flight_weather.describe()\n",
    "\n",
    "flight_weather = flight_weather[flight_weather[\"wind_speed\"] < 1000] #there's outlier that is probably error, because it's impossible for wind speed to be above 1000mph(The Highest Anemometer-Measured Wind Speeds on Earth, 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now trying to get rid of missing value I'm using regression imputing for it\n",
    "missing_columns = [\"temp\", \"dewp\", \"humid\", \"wind_dir\", \"wind_speed\",\"wind_gust\",\"precip\",\"pressure\",\"visib\"]\n",
    "#firstly random impute missing values - we have multiple variables with missing values so some predictors woud be nan and it wouldn't work\n",
    "def random_imputation(df, feature):\n",
    "\n",
    "    number_missing = df[feature].isnull().sum()\n",
    "    observed_values = df.loc[df[feature].notnull(), feature]\n",
    "    df.loc[df[feature].isnull(), feature + '_imp'] = np.random.choice(observed_values, number_missing, replace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "for feature in missing_columns:\n",
    "    flight_weather[feature + '_imp'] = flight_weather[feature]\n",
    "    flight_weather = random_imputation(flight_weather, feature)\n",
    "    \n",
    "deter_data = pd.DataFrame(columns = [\"Det\" + name for name in missing_columns])\n",
    "\n",
    "#using linear regression to input all the random imputed values with result of lin. regression for each feature with missing values\n",
    "for feature in missing_columns:\n",
    "        \n",
    "    deter_data[\"Det\" + feature] = flight_weather[feature + \"_imp\"]\n",
    "    parameters = list(set(flight_weather.columns) - set(missing_columns) - {feature + '_imp'})\n",
    "    model = LinearRegression()\n",
    "    model.fit(X = flight_weather[parameters], y = flight_weather[feature + '_imp'])\n",
    "    deter_data.loc[flight_weather[feature].isnull(), \"Det\" + feature] = model.predict(flight_weather[parameters])[flight_weather[feature].isnull()]\n",
    "    flight_weather[feature] = deter_data[\"Det\" + feature]\n",
    "    flight_weather = flight_weather.drop(columns=[feature + \"_imp\"])\n",
    "    \n",
    "\n",
    "flight_weather.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a688c",
   "metadata": {},
   "source": [
    "## OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    X = flight_weather.drop(columns=['dep_delay'])\n",
    "    y = flight_weather['dep_delay']\n",
    "    X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "    ODS = LinearRegression()\n",
    "    ODS.fit(X_train, y_train)\n",
    "\n",
    "    print(\"R^2 on train data is {} and on test data is {}\".format(ODS.score(X_train, y_train),ODS.score(X_test,y_test)))\n",
    "    print(\"the intercept is {}\".format(ODS.intercept_))\n",
    "    pd.Series(ODS.coef_, index=X.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b6704",
   "metadata": {},
   "source": [
    "#### Ploting OLS for each feature separately to see their correlation with dep delay time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRegression(feature,flight_weather):\n",
    "    #flight_weather[feature] = scaler.fit_transform(flight_weather[[feature]])\n",
    "    X = flight_weather[[feature]]\n",
    "    y = flight_weather['dep_delay']\n",
    "    X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "    ODS = LinearRegression()\n",
    "    ODS.fit(X_train, y_train)\n",
    "\n",
    "    print(\"R^2 on train data is {} and on test data is {}\".format(ODS.score(X_train, y_train),ODS.score(X_test,y_test)))\n",
    "    print(flight_weather[['dep_delay',feature]].corr())\n",
    "    \n",
    "    y_pred = ODS.predict(X)\n",
    "    plt.scatter(X, y,color = \"green\") \n",
    "    plt.plot(X[feature], y_pred,color = \"blue\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Delay\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b528e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in flight_weather.columns:\n",
    "    plotRegression(feature,flight_weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important to run this once only, if rerun needed then run data loading and preproccesing again\n",
    "#the correlation for month might depend on which month we start counting from, so trying to plot it trying different months\n",
    "flight_weather['month'] = flight_weather['month'] - 8\n",
    "flight_weather[flight_weather['month']<0] = flight_weather[flight_weather['month']<0] + 12\n",
    "plotRegression('month',flight_weather)\n",
    "#same for hour\n",
    "flight_weather['hour'] = flight_weather['hour'] - 5\n",
    "flight_weather[flight_weather['hour']<0] = flight_weather[flight_weather['hour']<0] + 24\n",
    "plotRegression('hour',flight_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1aaa2",
   "metadata": {},
   "source": [
    "## Ridge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(-10, 10, 100)\n",
    "\n",
    "for feature in flight_weather.columns:\n",
    "    flight_weather[feature] = scaler.fit_transform(flight_weather[[feature]])\n",
    "\n",
    "X = flight_weather.drop(columns=['dep_delay'])\n",
    "y = flight_weather['dep_delay']\n",
    "    \n",
    "ridge = Ridge(normalize = True)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha = a)\n",
    "    ridge.fit(X, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "    \n",
    "np.shape(coefs)\n",
    "temp_coefficients = [coef[0] for coef in coefs]\n",
    "dewp_coefficients = [coef[1] for coef in coefs]\n",
    "humid_coefficients = [coef[2] for coef in coefs]\n",
    "wind_dir_coefficients = [coef[3] for coef in coefs]\n",
    "wind_speed_coefficients = [coef[4] for coef in coefs]\n",
    "wind_gust_coefficients = [coef[5] for coef in coefs]\n",
    "precip_coefficients = [coef[6] for coef in coefs]\n",
    "pressure_coefficients = [coef[7] for coef in coefs]\n",
    "visib_coefficients = [coef[8] for coef in coefs]\n",
    "month_coefficients = [coef[9] for coef in coefs]\n",
    "day_coefficients = [coef[10] for coef in coefs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bffa41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.set_xscale('log')\n",
    "ax.plot(alphas, temp_coefficients, label=\"temp coefficient\")\n",
    "ax.plot(alphas, dewp_coefficients, label=\"dewp coefficient\")\n",
    "ax.plot(alphas, humid_coefficients, label=\"humid coefficient\")\n",
    "ax.plot(alphas, wind_dir_coefficients, label=\"wind_dir coefficient\")\n",
    "ax.plot(alphas, wind_speed_coefficients, label=\"wind_speed coefficient\")\n",
    "ax.plot(alphas, wind_gust_coefficients, label=\"wind_gust coefficient\")\n",
    "ax.plot(alphas, precip_coefficients, label=\"precip coefficient\")\n",
    "ax.plot(alphas, pressure_coefficients, label=\"pressure coefficient\")\n",
    "ax.plot(alphas, visib_coefficients, label=\"visib coefficient\")\n",
    "ax.plot(alphas, month_coefficients, label=\"month coefficient\")\n",
    "ax.plot(alphas, day_coefficients, label=\"day coefficient\")\n",
    "plt.legend()\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ef610",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "    X = flight_weather.drop(columns=['dep_delay'])\n",
    "    y = flight_weather['dep_delay']\n",
    "    X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "\n",
    "    ridgecv = RidgeCV( normalize = True)#alphas = alphas,\n",
    "    ridgecv.fit(X_train, y_train)\n",
    "    print(ridgecv.alpha_)   \n",
    "    print(\"R^2 on train data is {} and on test data is {}  \".format(ridgecv.score(X_train, y_train), ridgecv.score(X_test,y_test)))\n",
    "    pd.Series(ridgecv.coef_, index = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ba942",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "X = flight_weather.drop(columns=['dep_delay'])\n",
    "y = flight_weather['dep_delay']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "lassocv = LassoCV(cv = 10, max_iter = 100000, normalize = True)#alphas = None,\n",
    "lassocv.fit(X_train, y_train)\n",
    "print(lassocv.alpha_)\n",
    "print(\"R^2 on train data is {} and on test data is {}\".format(lassocv.score(X_train, y_train),lassocv.score(X_test,y_test)))\n",
    "pd.Series(lassocv.coef_, index=X.columns)\n",
    "\n",
    "#lasso.set_params(alpha=lassocv.alpha_)\n",
    "#lasso.fit(X_train, y_train)\n",
    "\n",
    "#print(\"R^2 on train data is {} and on test data is {}\".format(lassocv.score(X_train, y_train),lassocv.score(X_test,y_test)))\n",
    "#pd.Series(lasso.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b9c8e",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    X = flight_weather.drop(columns=['dep_delay'])\n",
    "    y = flight_weather['dep_delay']\n",
    "    X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "    #cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=69)\n",
    "    ratios = [.1, .2, .3 , .4, .5, .7, .8, .9, .95, .99, 1]\n",
    "    #alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0]\n",
    "    en = ElasticNetCV(l1_ratio=ratios, cv=10, n_jobs=-1)\n",
    "    en.fit(X_train, y_train)\n",
    "    print('alpha: %f' % en.alpha_)\n",
    "    print('l1_ratio_: %f' % en.l1_ratio_)\n",
    "    print(\"R^2 on train data is {} and on test data is {}\".format(en.score(X_train, y_train),en.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d79468d",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "After building regression on weather data with different models it's visible that the most 'influencial' parameter is hour. Top three are: hour, month and precip(now it depends on model). Weather conditions don't really influence deperture delay much therefore it's hard to build good regression model on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f598d147",
   "metadata": {},
   "source": [
    "**(f)** Is the age of the plane associated with delay? Do OLS, Ridge, Lasso, and Elastic Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NIKITA\n",
    "\n",
    "# DATA PREPARATION \n",
    "\n",
    "planes_flights = pd.merge(planes, flights, how='outer', left_on = 'tailnum', right_on = 'tailnum')\n",
    "planes_flights = planes_flights.drop(['type', 'manufacturer', 'seats', 'model', 'engines', 'speed', 'engine', 'carrier', 'flight', 'origin', 'dest', 'arr_time', 'dep_time', 'air_time', 'distance'], axis=1)\n",
    "planes_flights['years'] = planes_flights['year_y']-planes_flights['year_x']\n",
    "planes_flights['delay'] = planes_flights['dep_delay'] + (planes_flights['arr_delay'] - planes_flights['dep_delay'] )\n",
    "planes_flights = planes_flights.drop(['arr_delay', 'dep_delay', 'year_x', 'year_y', 'month', 'day', 'hour', 'minute'], axis=1)\n",
    "\n",
    "# taking average delay of each plane\n",
    "\n",
    "planes_flights = planes_flights.groupby(['tailnum']).mean()\n",
    "planes_flights = planes_flights.reset_index()\n",
    "planes_flights = planes_flights.drop(['tailnum'], axis=1)\n",
    "\n",
    "planes_flights = planes_flights.dropna(subset=['years'])\n",
    "\n",
    "planes_flights = planes_flights.sort_values(by=['delay'])\n",
    "\n",
    "#X = result['delay'].array.reshape(-1, 1)\n",
    "#y = result['years']\n",
    "\n",
    "#X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32dcad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now trying to get rid of missing value I'm using regression imputing for it\n",
    "missing_columns = [\"delay\"]\n",
    "#firstly random impute missing values - we have multiple variables with missing values so some predictors woud be nan and it wouldn't work\n",
    "def random_imputation(df, feature):\n",
    "\n",
    "    number_missing = df[feature].isnull().sum()\n",
    "    observed_values = df.loc[df[feature].notnull(), feature]\n",
    "    df.loc[df[feature].isnull(), feature + '_imp'] = np.random.choice(observed_values, number_missing, replace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "for feature in missing_columns:\n",
    "    planes_flights[feature + '_imp'] = planes_flights[feature]\n",
    "    planes_flights = random_imputation(planes_flights, feature)\n",
    "    \n",
    "deter_data = pd.DataFrame(columns = [\"Det\" + name for name in missing_columns])\n",
    "\n",
    "#using linear regression to input all the random imputed values with result of lin. regression for each feature with missing values\n",
    "for feature in missing_columns:\n",
    "        \n",
    "    deter_data[\"Det\" + feature] = planes_flights[feature + \"_imp\"]\n",
    "    parameters = list(set(planes_flights.columns) - set(missing_columns) - {feature + '_imp'})\n",
    "    model = LinearRegression()\n",
    "    model.fit(X = planes_flights[parameters], y = planes_flights[feature + '_imp'])\n",
    "    deter_data.loc[planes_flights[feature].isnull(), \"Det\" + feature] = model.predict(planes_flights[parameters])[planes_flights[feature].isnull()]\n",
    "    planes_flights[feature] = deter_data[\"Det\" + feature]\n",
    "    planes_flights = planes_flights.drop(columns=[feature + \"_imp\"])\n",
    "    \n",
    "\n",
    "planes_flights.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc48adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of years/delay\n",
    "\n",
    "print(planes_flights[['years','delay']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c5a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRegression(X_train,y_train):\n",
    "    plt.scatter(X_train, y_train, color='red') # plotting the observation line\n",
    "    plt.plot(X_train, lr.predict(X_train), color='blue') # plotting the regression line\n",
    "    plt.title(\"(Training set)\") # stating the title of the graph\n",
    "    plt.xlabel(\"Delay\") # adding the name of x-axis\n",
    "    plt.ylabel(\"Ages\") # adding the name of y-axis\n",
    "    plt.show() # specifies end of graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a7891",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS\n",
    "\n",
    "X = planes_flights['delay'].array.reshape(-1, 1)\n",
    "y = planes_flights['years']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"lr.coef_: {}\".format(lr.coef_))\n",
    "print(\"lr.intercept_: {}\".format(lr.intercept_))\n",
    "print(\"Training set score (R2): {:.6f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.6f}\".format(lr.score(X_test, y_test)))\n",
    "print('Difference in training and test set score signs of overfiting')\n",
    "\n",
    "## making graph\n",
    "plotRegression(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge\n",
    "alphas = 10**np.linspace(-10, 10, 100)\n",
    "ridge = Ridge(normalize = True)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha = a)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "np.shape(coefs)\n",
    "_coefficients = [coef[0] for coef in coefs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb729d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.set_xscale('log')\n",
    "ax.plot(alphas, _coefficients, label=\"delay coefficient\")\n",
    "plt.legend()\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39657c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = planes_flights['delay'].array.reshape(-1, 1)\n",
    "y = planes_flights['years']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "ridgecv = RidgeCV(normalize = True)#alphas = alphas,\n",
    "ridgecv.fit(X_train, y_train)\n",
    "print(ridgecv.alpha_)   \n",
    "print(\"R^2 on train data is {} and on test data is {}  \".format(ridgecv.score(X_train, y_train), ridgecv.score(X_test,y_test)))\n",
    "\n",
    "## making graph\n",
    "plotRegression(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3deff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso\n",
    "X = planes_flights['delay'].array.reshape(-1, 1)\n",
    "y = planes_flights['years']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "#we increase the default setting of \"max_iter\",\n",
    "#otherwise the model would warn us that we should increase max_iter.\n",
    "lasso = Lasso(alpha=0.01, max_iter=10000, normalize = True).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.6f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.6f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "## making graph\n",
    "plotRegression(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40958183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LassoCV\n",
    "X = planes_flights['delay'].array.reshape(-1, 1)\n",
    "y = planes_flights['years']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "lassocv = LassoCV(cv = 10, max_iter = 100000, normalize = True)#alphas = None,\n",
    "lassocv.fit(X_train, y_train)\n",
    "print(lassocv.alpha_)\n",
    "print(\"R^2 on train data is {} and on test data is {}\".format(lassocv.score(X_train, y_train),lassocv.score(X_test,y_test)))\n",
    "\n",
    "## making graph\n",
    "plotRegression(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c985aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "X = planes_flights['delay'].array.reshape(-1, 1)\n",
    "y = planes_flights['years']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "#cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=69)\n",
    "ratios = [.1, .2, .3 , .4, .5, .7, .8, .9, .95, .99, 1]\n",
    "#alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0]\n",
    "en = ElasticNetCV(l1_ratio=ratios, cv=10, n_jobs=-1)\n",
    "en.fit(X_train, y_train)\n",
    "\n",
    "print('alpha: %f' % en.alpha_)\n",
    "print('l1_ratio_: %f' % en.l1_ratio_)\n",
    "print(\"R^2 on train data is {} and on test data is {}\".format(en.score(X_train, y_train),en.score(X_test,y_test)))\n",
    "\n",
    "## making graph\n",
    "plotRegression(X_train,y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33ca7e",
   "metadata": {},
   "source": [
    "**(g)** Do a principal component analysis of the weather at JFK using the following columns:\n",
    "   temp, dewp, humid, wind_dir, wind_speed, precip, visib.\n",
    "   How many principal components should be used to capture the variability in the weather data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nikita \n",
    "features = ['temp', 'dewp', 'humid', 'wind_dir', 'wind_speed', 'precip', 'visib']\n",
    "weather_to_use = weather.dropna()\n",
    "\n",
    "display(weather_to_use)\n",
    "\n",
    "# Separating out the features\n",
    "X = weather_to_use.loc[:, features].values\n",
    "display(X)\n",
    "# Separating out the origin\n",
    "y = weather_to_use['origin'].values\n",
    "display(y)\n",
    "# Splitting the X and Y into the\n",
    "sc = StandardScaler()\n",
    "# Standardizing the features\n",
    "sc.fit(X)\n",
    "X_scaled = sc.transform(X)\n",
    "\n",
    "# Applying PCA \n",
    "# 7 components sa the number of features we have\n",
    "pca = decomposition.PCA(n_components = 7)\n",
    "pca.fit(X_scaled)\n",
    "X_pca_7 = pca.transform(X_scaled)\n",
    "\n",
    "print(\"variance explained by all 7 principal components =\", sum(pca.explained_variance_ratio_ * 100))\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_ * 100))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.savefig('elbow_plot.png', dpi=100)\n",
    "\n",
    "print(\"variance explained by the First principal component =\", \n",
    "      np.cumsum(pca.explained_variance_ratio_ * 100)[0])\n",
    "\n",
    "print(\"variance explained by the 2 principal component =\", \n",
    "      np.cumsum(pca.explained_variance_ratio_ * 100)[1])\n",
    "\n",
    "print(\"variance explained by the 3 principal component =\", \n",
    "      np.cumsum(pca.explained_variance_ratio_ * 100)[2])\n",
    "\n",
    "print(\"variance explained by the 4 principal component =\", \n",
    "      np.cumsum(pca.explained_variance_ratio_ * 100)[3])\n",
    "\n",
    "print(\"variance explained by the 5 principal component =\", \n",
    "      np.cumsum(pca.explained_variance_ratio_ * 100)[4])\n",
    "\n",
    "print(\"variance explained by the 6 principal component =\", \n",
    "      np.cumsum(pca.explained_variance_ratio_ * 100)[5])\n",
    "\n",
    "print(\"variance explained by the 7 principal component =\", \n",
    "      np.cumsum(pca.explained_variance_ratio_ * 100)[6])\n",
    "\n",
    "# It is not possible to create a scatterplot for the original datase because it contains 7 features.\n",
    "# We can make 2D and 3D scatterplots, but in this case we will lose some variability\n",
    "# For 2D scatterplot we will have 56.6...% variability\n",
    "# For 3D scatterplot we will have 72.2...% variability \n",
    "\n",
    "# Variance after 5 porincipal componenet are not changing significantlty \n",
    "\n",
    "# Select the best number of principal components while keeping as much of the variance in the original data as possible.\n",
    "\n",
    "# We can choose 5 prncipal components to capture the variability in the weather data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43234aad",
   "metadata": {},
   "source": [
    "**(h)** Build regression models (OLS, Ridge, Lasso, and Elastic Net) that associates\n",
    "   an airports lattitude with weather conditions (temp, dewp, humid, wind_dir, wind_speed,\n",
    "   precip, visib). Remove all but the three most significant whether conditions and redo\n",
    "   the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f2af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nikita \n",
    "# Data Preparation\n",
    "data = pd.merge(airports, weather, how='outer', left_on = 'faa', right_on = 'origin')\n",
    "data = data.drop(columns = ['year', 'month', 'day', 'hour','lon', 'alt', 'tz', 'dst', 'tzone', 'origin', 'name', 'time_hour', 'pressure', 'wind_gust', 'faa'])\n",
    "display(data)\n",
    "data = data.dropna(subset=['lat'])\n",
    "data = data[data[\"wind_speed\"] < 1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f22041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now trying to get rid of missing value I'm using regression imputing for it\n",
    "missing_columns = [\"temp\", \"dewp\", \"humid\", \"wind_dir\", \"wind_speed\",\"precip\",\"visib\"]\n",
    "#firstly random impute missing values - we have multiple variables with missing values so some predictors woud be nan and it wouldn't work\n",
    "\n",
    "for feature in missing_columns:\n",
    "    data[feature + '_imp'] = data[feature]\n",
    "    data = random_imputation(data, feature)\n",
    "    \n",
    "deter_data = pd.DataFrame(columns = [\"Det\" + name for name in missing_columns])\n",
    "\n",
    "#using linear regression to input all the random imputed values with result of lin. regression for each feature with missing values\n",
    "for feature in missing_columns:\n",
    "        \n",
    "    deter_data[\"Det\" + feature] = data[feature + \"_imp\"]\n",
    "    parameters = list(set(data.columns) - set(missing_columns) - {feature + '_imp'})\n",
    "    model = LinearRegression()\n",
    "    model.fit(X = data[parameters], y = data[feature + '_imp'])\n",
    "    deter_data.loc[data[feature].isnull(), \"Det\" + feature] = model.predict(data[parameters])[data[feature].isnull()]\n",
    "    data[feature] = deter_data[\"Det\" + feature]\n",
    "    data = data.drop(columns=[feature + \"_imp\"])\n",
    "    \n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25898f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS \n",
    "\n",
    "X = data.loc[:,'temp':'visib']\n",
    "y = data['lat']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"R^2 on train data is {} and on test data is {}\".format(lr.score(X_train, y_train),lr.score(X_test,y_test)))\n",
    "print(\"the intercept is {}\".format(lr.intercept_))\n",
    "pd.Series(lr.coef_, index=X.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ff878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotOLSRegression(feature, data):\n",
    "    X = data[[feature]]\n",
    "    y = data['lat']\n",
    "    X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    print(\"R^2 on train data is {} and on test data is {}\".format(lr.score(X_train, y_train),lr.score(X_test,y_test)))\n",
    "    print(data[['lat',feature]].corr())\n",
    "    \n",
    "    y_pred = lr.predict(X)\n",
    "    plt.scatter(X,y,color = \"green\") \n",
    "    plt.plot(X[feature], y_pred,color = \"blue\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Lat\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS\n",
    "\n",
    "for feature in data.columns:\n",
    "    plotOLSRegression(feature,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge\n",
    "\n",
    "alphas = 10**np.linspace(-10, 10, 100)\n",
    "\n",
    "X = data.loc[:,'temp':'visib']\n",
    "y = data['lat']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "ridge = Ridge(normalize = True)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha = a)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "    \n",
    "np.shape(coefs)\n",
    "temp_coefficients = [coef[0] for coef in coefs]\n",
    "dewp_coefficients = [coef[1] for coef in coefs]\n",
    "humid_coefficients = [coef[2] for coef in coefs]\n",
    "wind_dir_coefficients = [coef[3] for coef in coefs]\n",
    "wind_speed_coefficients = [coef[4] for coef in coefs]\n",
    "precip_coefficients = [coef[5] for coef in coefs]\n",
    "visib_coefficients = [coef[6] for coef in coefs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03573ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.set_xscale('log')\n",
    "ax.plot(alphas, temp_coefficients, label=\"temp coefficient\")\n",
    "ax.plot(alphas, dewp_coefficients, label=\"dewp coefficient\")\n",
    "ax.plot(alphas, humid_coefficients, label=\"humid coefficient\")\n",
    "ax.plot(alphas, wind_dir_coefficients, label=\"wind_dir coefficient\")\n",
    "ax.plot(alphas, wind_speed_coefficients, label=\"wind_speed coefficient\")\n",
    "ax.plot(alphas, precip_coefficients, label=\"precip coefficient\")\n",
    "ax.plot(alphas, visib_coefficients, label=\"visib coefficient\")\n",
    "plt.legend()\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RidgeCV\n",
    "\n",
    "X = data.loc[:,'temp':'visib']\n",
    "y = data['lat']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "\n",
    "ridgecv = RidgeCV(normalize = True)\n",
    "ridgecv.fit(X_train, y_train)\n",
    "print(ridgecv.alpha_)   \n",
    "print(\"R^2 on train data is {} and on test data is {}  \".format(ridgecv.score(X_train, y_train), ridgecv.score(X_test,y_test)))\n",
    "pd.Series(ridgecv.coef_, index = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa71284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso\n",
    "\n",
    "X = data.loc[:,'temp':'visib']\n",
    "y = data['lat']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "lasso = Lasso(alpha = 8.3,max_iter = 10000, normalize = True)#alphas = None,\n",
    "lasso.fit(X_train, y_train)\n",
    "print(\"R^2 on train data is {} and on test data is {}\".format(lasso.score(X_train, y_train),lasso.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LassoCV\n",
    "\n",
    "X = data.loc[:,'temp':'visib']\n",
    "y = data['lat']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "\n",
    "lassocv = LassoCV(cv = 10, max_iter = 100000, normalize = True)#alphas = None,\n",
    "lassocv.fit(X_train, y_train)\n",
    "print(lassocv.alpha_)\n",
    "print(\"R^2 on train data is {} and on test data is {}\".format(lassocv.score(X_train, y_train),lassocv.score(X_test,y_test)))\n",
    "\n",
    "pd.Series(lassocv.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net CV\n",
    "\n",
    "X = data.loc[:,'temp':'visib']\n",
    "y = data['lat']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "\n",
    "#cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=69)\n",
    "ratios = [.1, .2, .3 , .4, .5, .7, .8, .9, .95, .99, 1]\n",
    "#alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0]\n",
    "en = ElasticNetCV(l1_ratio=ratios, cv=10, n_jobs=-1)\n",
    "en.fit(X_train, y_train)\n",
    "print('alpha: %f' % en.alpha_)\n",
    "print('l1_ratio_: %f' % en.l1_ratio_)\n",
    "print(\"R^2 on train data is {} and on test data is {}\".format(en.score(X_train, y_train),en.score(X_test, y_test)))\n",
    "pd.Series(en.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f79683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all but the three most significant whether conditions and redo the analysis.\n",
    "# temp, dewp, humid, wind_dir, wind_speed, precip, visib\n",
    "\n",
    "# dewp, precip, visib have more significant change after doing ElasticNetCV \n",
    "\n",
    "data_upd = data.drop(['temp', 'wind_speed', 'humid', 'wind_dir'], axis=1)\n",
    "\n",
    "data_upd\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac46d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS\n",
    "X = data_upd.loc[:,'dewp':'visib']\n",
    "y = data_upd['lat']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"R^2 on train data is {} and on test data is {}\".format(lr.score(X_train, y_train),lr.score(X_test,y_test)))\n",
    "print(\"the intercept is {}\".format(lr.intercept_))\n",
    "pd.Series(lr.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in data_upd.columns:\n",
    "    plotOLSRegression(feature,data_upd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac95c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge\n",
    "\n",
    "X = data_upd.loc[:,'dewp':'visib']\n",
    "y = data_upd['lat']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "\n",
    "ridgecv = RidgeCV(normalize = True)\n",
    "ridgecv.fit(X_train, y_train)\n",
    "print(ridgecv.alpha_)   \n",
    "print(\"R^2 on train data is {} and on test data is {}  \".format(ridgecv.score(X_train, y_train), ridgecv.score(X_test,y_test)))\n",
    "pd.Series(ridgecv.coef_, index = X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af4061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso\n",
    "\n",
    "X = data_upd.loc[:,'dewp':'visib']\n",
    "y = data_upd['lat']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "\n",
    "lassocv = LassoCV(cv = 10, max_iter = 100000, normalize = True)#alphas = None,\n",
    "lassocv.fit(X_train, y_train)\n",
    "print(lassocv.alpha_)\n",
    "print(\"R^2 on train data is {} and on test data is {}\".format(lassocv.score(X_train, y_train),lassocv.score(X_test,y_test)))\n",
    "\n",
    "pd.Series(lassocv.coef_, index=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net\n",
    "X = data_upd.loc[:,'dewp':'visib']\n",
    "y = data_upd['lat']\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "\n",
    "#cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=69)\n",
    "ratios = [.1, .2, .3 , .4, .5, .7, .8, .9, .95, .99, 1]\n",
    "#alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0]\n",
    "en = ElasticNetCV(l1_ratio=ratios, cv=10, n_jobs=-1)\n",
    "en.fit(X_train, y_train)\n",
    "print('alpha: %f' % en.alpha_)\n",
    "print('l1_ratio_: %f' % en.l1_ratio_)\n",
    "print(\"R^2 on train data is {} and on test data is {}\".format(en.score(X_train, y_train),en.score(X_test, y_test)))\n",
    "pd.Series(en.coef_, index=X.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835084f6",
   "metadata": {},
   "source": [
    "**(i)** On a map, plot the airports that have flights to them where the points that represent\n",
    "   airports are relative in size to the average departure delay. You can see an example in \"airports.png\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e97c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nikita\n",
    "\n",
    "data_map = pd.merge(airports, flights, how='outer', left_on = 'faa', right_on = 'dest')\n",
    "data_map = data_map.dropna()\n",
    "\n",
    "data_map = data_map.drop(['dep_time', 'arr_time', 'arr_delay', 'carrier', 'tailnum', 'flight', 'origin', 'dest', 'air_time', 'distance', 'tzone', 'tz', 'alt', 'name', 'faa', 'hour', 'minute', 'year', 'month', 'day', 'dst'], axis=1)\n",
    "\n",
    "data_map = data_map.groupby(['lat', 'lon']).mean()\n",
    "\n",
    "data_map = data_map.reset_index()\n",
    "\n",
    "positive_delays = data_map[(data_map['dep_delay']>0)]\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(positive_delays['lon'], positive_delays['lat'])]\n",
    "gdf = GeoDataFrame(positive_delays, geometry=geometry)   \n",
    "\n",
    "#this is a simple map that goes with geopandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "gdf.plot(ax=world.plot(figsize=(10, 6)), marker='o', color='red', markersize=positive_delays['dep_delay'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d904bc39",
   "metadata": {},
   "source": [
    " **(j)** These questions require no code.\n",
    " - Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter or reduce it?\n",
    "\n",
    "- Why would you want to use:\n",
    "        > Ridge Regression instead of plain Linear Regression (i.e. without any regularization)?\n",
    "        > Lasso instead of Ridge Regression?\n",
    "        > Elastic Net instead of Lasso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9a3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nikita\n",
    "# Suppose you are using Ridge Regression and you notice that the training error and the validation \n",
    "# error are almost equal and fairly high. Would you say that the model suffers from high bias or high \n",
    "# variance? Should you increase the regularization hyperparameter or reduce it?\n",
    "\n",
    "# When the training error and validation error are close to each other and high that means your model \n",
    "# is underfitting (i.e. it has high bias). You should try to reduce the regularization hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98bc388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why would you want to use:\n",
    "\n",
    "#Ridge Regression instead of plain Linear Regression (i.e. without any regularization)?\n",
    "\n",
    "#When you have features in your dataset that are highly linearly correlated with other features, \n",
    "#turns out linear models will be likely to overfit. Ridge Regression, avoids over fitting \n",
    "#by adding a penalty to models that have too large coefficients.\n",
    "\n",
    "#Lasso instead of Ridge Regression?\n",
    "\n",
    "#Lasso tends to do well if there are a small number of significant parameters and the others \n",
    "#are close to zero.\n",
    "\n",
    "# Elastic Net instead of Lasso?\n",
    "\n",
    "#Elastic Net combines feature elimination from Lasso and feature coefficient reduction from \n",
    "#the Ridge model to improve your model's predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c092b",
   "metadata": {},
   "source": [
    "## 3. Clustering of Handwritten Digits\n",
    "\n",
    "__You should be able to do this exercise after Lecture 5.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc371258",
   "metadata": {},
   "source": [
    "This exercise will depart from the famous MNIST dataset, and we are exploring several clustering techniques with it.. This is a \".mat\" file, in order to load this file in an ipynb you have to use loadmat() function from scipy.io. (replace my path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import sklearn.utils as utils\n",
    "from scipy.io import loadmat\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "mnist = loadmat('datasets/3/mnist-original')\n",
    "mnist_data = mnist[\"data\"].T\n",
    "mnist_label = mnist[\"label\"][0]\n",
    "unique_labels = np.unique(mnist_label)\n",
    "\n",
    "print(\"Number of datapoints: {}\\n\".format(mnist_data.shape[0]))\n",
    "print(\"Number of features: {}\\n\".format(mnist_data.shape[1]))\n",
    "print(\"List of labels: {}\\n\".format(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa51aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's perform feature selection for each class separately\n",
    "def find_indices(lst, condition):\n",
    "    return [i for i, elem in enumerate(lst) if condition(elem)]\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for label in unique_labels:\n",
    "    indeces_by_label = find_indices(mnist_label, lambda e: e == label)\n",
    "    \n",
    "    data_for_label = mnist_data[indeces_by_label]\n",
    "    variance_thr = VarianceThreshold().fit(data_for_label)\n",
    "    selection_mask = variance_thr.get_support()\n",
    "    \n",
    "    print(\"Applying selection mask for class\", int(label), \"this may take some time...\")\n",
    "    # go through each instance of this class\n",
    "    for i in indeces_by_label:\n",
    "        img = mnist_data[i]\n",
    "        new_img = []\n",
    "        # go through each pixel of img\n",
    "        for idx, px in enumerate(img):\n",
    "            if(selection_mask[idx]): # if pixel was selected by the selection mask\n",
    "                new_img.append(round(px / 255, 4)) # scale the pixel and move on\n",
    "            else:\n",
    "                new_img.append(0) # clear the pixel\n",
    "        X.append(np.array(new_img))\n",
    "        Y.append(label)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test , y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, stratify=Y, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575f2c4",
   "metadata": {},
   "source": [
    "There are 70,000 images, and each image has 784 features. This is because each image is 28×28 pixels,\n",
    "and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black). Let’s take a peek at one digit from the dataset. All you need to do is grab an instance’s feature vector, reshape it to a 28×28 array, and display it using Matplotlib’s `imshow()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14d40f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "index = 123\n",
    "print(\"Label of datapoint =\", int(y_train[index]))\n",
    "print(\"Value of datapoint:\\n\", X_train[index])\n",
    "print(\"As image:\\n\")\n",
    "plt.imshow(X_train[index].reshape(28,28), cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb554a7d",
   "metadata": {},
   "source": [
    "**(a)** Perform k-means clustering with k=10 on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11980e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_clusters = KMeans(n_clusters=10, random_state=69).fit(X_train)\n",
    "\n",
    "accuracy = accuracy_score(y_train, raw_clusters.labels_);\n",
    "print(\n",
    "    \"The accuracy is:\",\n",
    "    accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "print(\"True label of datapoint =\", int(y_train[index]))\n",
    "print(\"Predicted label of datapoint =\", int(raw_clusters.labels_[index]))\n",
    "print(\"As image:\\n\")\n",
    "plt.imshow(X_train[index].reshape(28,28), cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704f2fbf",
   "metadata": {},
   "source": [
    "The accuracy is very low, so let's perform some feature selection. Let's remove the features that do not have much variance througout the data. To find the best Variance Threshold, let's go over a small part of the data with a loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a796e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "i = 0\n",
    "best_threshold = i\n",
    "best_clusters = 0;\n",
    "selected_features = []\n",
    "selection_mask = [];\n",
    "\n",
    "while(True):\n",
    "    try:\n",
    "        variance_thr = VarianceThreshold(i).fit(X_train)\n",
    "        x = variance_thr.fit_transform(X_train)\n",
    "        clusters = KMeans(n_clusters=10, random_state=69).fit(x)\n",
    "    except: # it throuws an error when the threshold is too large for the data\n",
    "        break\n",
    "\n",
    "    accuracy = accuracy_score(y_train, clusters.labels_);\n",
    "    \n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = i\n",
    "        best_clusters = clusters\n",
    "        selection_mask = variance_thr.get_support()\n",
    "        selected_features = x\n",
    "    \n",
    "    i = round(i + 0.01, 2)\n",
    "    \n",
    "print(\n",
    "    \"The best accuracy of\", best_accuracy,\n",
    "    \"resulted from the threshold of\", best_threshold,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 7\n",
    "print(\"True label of datapoint =\", int(y_train[index]))\n",
    "print(\"Predicted label of datapoint =\", int(best_clusters.labels_[index]))\n",
    "print(\"As image:\\n\")\n",
    "plt.imshow(X_train[index].reshape(28,28), cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4afa3e5",
   "metadata": {},
   "source": [
    "**(b)** Using visualization techniques analogous to what we have done in the Clustering notebook\n",
    "   for the faces data, can you determine the 'nature' of the 10 constructed clusters?\n",
    "   Do the clusters (roughly) coincide with the 10 different actual digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e046e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2,5, figsize=(15, 8))\n",
    "for ax,cc,i in zip (axes.ravel(),best_clusters.cluster_centers_,np.arange(axes.ravel().size)):\n",
    "    ax.set_title(\"digit: {}, size: {}\".format(i,len(np.where(best_clusters.labels_==i)[0])))\n",
    "    selected_pic = cc\n",
    "    full_pic = []\n",
    "    for j in range(0, 784):\n",
    "        if(selection_mask[j]):\n",
    "            full_pic.append(float(selected_pic[0]))\n",
    "            selected_pic = selected_pic[1:]\n",
    "        else:\n",
    "            full_pic.append(float(0))   \n",
    "    ax.imshow(np.array(full_pic).reshape(28,28),cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc3cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = best_clusters.transform(selected_features)\n",
    "\n",
    "pseudo_medoids=np.argmin(distances,axis=0)\n",
    "fig,axes = plt.subplots(2,5,figsize=(15,8))\n",
    "for ax,pm,i in zip (axes.ravel(),pseudo_medoids,np.arange(axes.ravel().size)):\n",
    "    ax.set_title(i)\n",
    "    ax.imshow(X_train[pm].reshape(28,28),cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109f251",
   "metadata": {},
   "source": [
    "**(c)** Perform a supervised clustering evaluation using adjusted rand index.\n",
    "   Are the results stable, when you perform several random restarts of k-means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4d3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rand_index in range(1,10):\n",
    "    clusters = KMeans(n_clusters=10, init='random', n_init=10, random_state = rand_index).fit(X_train)\n",
    "    labels = clusters.labels_\n",
    "    accuracy = accuracy_score(y_train, labels);\n",
    "    print(\n",
    "        \"For rand_index =\",\n",
    "        rand_index,\n",
    "        \"; accuracy = \",\n",
    "        accuracy\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab3535",
   "metadata": {},
   "source": [
    "**(d)** Now perform hierarchical clustering on the data.\n",
    "   (in order to improve visibility in the constructed dendrograms, you can also use a\n",
    "   much reduced dataset as constructed using sklearn.utils.resample shown below).\n",
    "   Does the visual analysis of the dendrogram indicate a natural number of clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eba905",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_mnist_data, small_mnist_label = utils.resample(mnist_data, mnist_label, n_samples=200,replace='false')\n",
    "\n",
    "linkage_data = linkage(small_mnist_data, method='ward', metric='euclidean')\n",
    "dendrogram(linkage_data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9340bd9",
   "metadata": {},
   "source": [
    "**(e)** Using different cluster distance metrics (ward,single,average, etc.),\n",
    "   what do the clusterings look like that are produced at the level of k=10 clusters?\n",
    "   See the Clustering notebook for the needed Python code, including the fcluster\n",
    "   method to retrieve 'plain' clusterings from the hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_mnist_data,small_mnist_label = skl.utils.resample(mnist.data,mnist.target,n_samples=200,replace='false')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f07c49",
   "metadata": {},
   "source": [
    "**(f)** Do a DBSCAN clustering of the small dataset. Tweak the different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCluster(clusters, X_scaled):\n",
    "    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, s=60)\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")\n",
    "    print(\"Unique groups: {}\".format(np.unique(clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af64a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X = normalized_images\n",
    "y = small_mnist_label\n",
    "# rescale the data to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda7821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# epsilon - local radius for expanding clusters\n",
    "# min_samples - the fewest number of points required to form a cluster\n",
    "# unique groups show an array of unique formed groups. If array containing -1 number, means there is noise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clustering without parameters / default epsilon is 0.5 and min_samples are 4\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "# plot the cluster assignments\n",
    "plotCluster(clusters, X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1676248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon 0.5 is too big. So it's forming only one group \n",
    "# DBSCAN clustering with eps 0.3 and default min_samples = 4\n",
    "dbscan = DBSCAN(eps = 0.3)\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "# plot the cluster assignments\n",
    "plotCluster(clusters, X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now it's possible to see two groups and one sample of noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b42d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clustering with eps 0.4 and min_samples = 3\n",
    "dbscan = DBSCAN(eps = 0.4, min_samples = 3)\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "# plot the cluster assignments\n",
    "plotCluster(clusters, X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a634918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With eps 0.4 and min_sample 3 we can see two groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clustering with eps 0.01 and min_samples = 1\n",
    "dbscan = DBSCAN(eps = 0.01, min_samples = 1)\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "# plot the cluster assignments\n",
    "plotCluster(clusters, X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(random_state=0, n_samples=100)\n",
    "\n",
    "# rescale the data to zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With low eps 0.01 and minimum sample to form group we can see that each sample form it's own group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e77eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clustering with default parameters on blob map\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "# plot the cluster assignments\n",
    "plotCluster(clusters, X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bd6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we can see 2 groups and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d62fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clustering with eps 0.2\n",
    "dbscan = DBSCAN(eps = 0.2)\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "# plot the cluster assignments\n",
    "plotCluster(clusters, X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After decresing eps value we have two small groups and a lot of noise areound. Epsilon is to small so the radius distance is not enough to form groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea706a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clustering with eps 0.6\n",
    "dbscan = DBSCAN(eps = 0.6)\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "# plot the cluster assignments\n",
    "plotCluster(clusters, X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a133be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After incresing epsilon, it is possible to see one big group with the noise around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9657af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clustering with eps 0.6 and min_samples = 1\n",
    "dbscan = DBSCAN(eps = 0.6, min_samples = 1)\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "# plot the cluster assignments\n",
    "plotCluster(clusters, X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I decreased number of samples to form group to one. Now we have 5 groups and no noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c004c",
   "metadata": {},
   "source": [
    "**(g)** Try to compare the different clustering methods on the MNIST dataset in the same way\n",
    "   the book does on the faces dataset on pp. 195-206."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "X = np.array(normalized_images)\n",
    "labels = mnist_label\n",
    "\n",
    "pca = PCA(n_components=100, whiten=True, random_state=0)\n",
    "pca.fit_transform(X)\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "print(\"Unique labels: {}\".format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469bcd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(min_samples=3)\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "print(\"Unique labels: {}\".format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3a64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(min_samples=3, eps=13)\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "print(\"Unique labels: {}\".format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cdc65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of points per cluster: {}\".format(np.bincount(labels + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deaeabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = X[labels==-1]\n",
    "\n",
    "fig, axes = plt.subplots(3, 9, subplot_kw={'xticks': (), 'yticks': ()},\n",
    " figsize=(12, 4))\n",
    "for image, ax in zip(noise, axes.ravel()):\n",
    " ax.imshow(image.reshape(28,28),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fb1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in [1, 3, 5, 7, 9, 11, 12]:\n",
    " print(\"\\neps={}\".format(eps))\n",
    " dbscan = DBSCAN(eps=eps, min_samples=3)\n",
    " labels = dbscan.fit_predict(X_pca)\n",
    " print(\"Clusters present: {}\".format(np.unique(labels)))\n",
    " print(\"Cluster sizes: {}\".format(np.bincount(labels + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(min_samples=3, eps=12)\n",
    "labels = dbscan.fit_predict(X_pca)\n",
    "for cluster in range(max(labels) + 1):\n",
    " mask = labels == cluster\n",
    " n_images = np.sum(mask)\n",
    " fig, axes = plt.subplots(1, n_images, figsize=(n_images * 1.5, 4),\n",
    " subplot_kw={'xticks': (), 'yticks': ()})\n",
    " for image, label, ax in zip(X_people[mask], y_people[mask], axes):\n",
    "     ax.imshow(image.reshape(28,28), vmin=0, vmax=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b4593",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=10, random_state=0)\n",
    "labels_km = km.fit_predict(X_pca)\n",
    "print(\"Cluster sizes k-means: {}\".format(np.bincount(labels_km)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d277799",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, subplot_kw={'xticks': (), 'yticks': ()},\n",
    " figsize=(12, 4))\n",
    "for center, ax in zip(km.cluster_centers_, axes.ravel()):\n",
    " ax.imshow(pca.inverse_transform(center).reshape(28,28),\n",
    " vmin=0, vmax=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fba1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# extract clusters with ward agglomerative clustering\n",
    "agglomerative = AgglomerativeClustering(n_clusters=10)\n",
    "labels_agg = agglomerative.fit_predict(X_pca)\n",
    "print(\"Cluster sizes agglomerative clustering: {}\".format(\n",
    " np.bincount(labels_agg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd3830",
   "metadata": {},
   "source": [
    "## 4. The Local Elections\n",
    "\n",
    "__You should be able to do this exercise after Lecture 6.__\n",
    "\n",
    "In the local elections of 2021, around 100 candidates stood for election for the city council of Horsens. 83 of them represented a national party, had more than one candidate and provided answers to the <a href=\"https://www.dr.dk/nyheder/politik/kandidattest\">DR Candidate Test</a>, a test designed to help voters find out who they should vote for. In this test, the candidates answered 18 questions, which we will use as features in the following. The politicians belong to 9 parties, which will be our classes.\n",
    "\n",
    "The numpy files `X_Horsens.npy` and `Y_Horsens.npy` contains the data. `Y_Horsens.npy` contains a letter representing the party to which each candidate belongs. The following parties are represented:\n",
    "\n",
    "| Party letter | Party name | Party name (English) | Political position | Party color |\n",
    "| :-: | :-: | :-: | :-: | :-: |\n",
    "| A | Socialdemokratiet | Social Democrats | Centre-left | Red |\n",
    "| B | Radikale Venstre | Social Liberal Party | Centre-left | Indigo |\n",
    "| C | Det Konservative Folkeparti | Conservative People's Party | Right-wing | Green |\n",
    "| D | Nye Borgerlige | New Right | Far-right | Black |\n",
    "| F | Socialistisk Folkeparti | Socialist People's Party | Left-wing | Fuchsia |\n",
    "| I | Liberal Alliance | Liberal Alliance | Right-wing | Cyan |\n",
    "| O | Dansk Folkeparti | Danish People's Party | Far-right | Yellow |\n",
    "| V | Venstre | Danish Liberal Party | Centre-right | Blue |\n",
    "| Z* | Enhedslisten | Red-Green Alliance | Far-left | Dark red |\n",
    "\n",
    "*_Note that, although the party letter of Enhedslisten is actually Ø, we will here use Z to avoid any complications with the wonderful Danish letters Æ, Ø and Å. Feel free to change the Z back to an Ø if you find that it does not cause any problems._\n",
    "\n",
    "Meanwhile, `X_Horsens.npy` contains the answers to the test as numbers between -1.5 and 1.5, such that -1.5 is \"Strongly disagree\", -0.5 is \"Disagree\", 0.5 is \"Agree\" and 1.5 is \"Strongly agree\". The 18 questions concern, in order, subdivision, schools, windmills, building permits, tall buildings, housing, child care, culture, nursing homes, taxes, sports, refugees, nursing homes (again), public transportation, meat-free days, welfare, privatization, and religious minorities.\n",
    "\n",
    "Both files can be imported using `numpy.load`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928c2d6",
   "metadata": {},
   "source": [
    "__(a)__ How well do you (intuitively) expect that we can predict the partisan affiliation of a candidate based on their answers to the test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2331d5",
   "metadata": {},
   "source": [
    "Julia  \n",
    "It's supervised classification with 9 classes. Model might have problem classifing politicians from parties ideologically simillar. Also dataset isn't big, with 100 entries and 9 classes there might be not enough of data for model to learn well enough. It might be around 50-60%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfefd89d",
   "metadata": {},
   "source": [
    "__(b)__ Based on the answers from all 83 candidates for the Horsens city council, perform a Principal Component Analysis with 2 principal components. Plot the results in a figure using these 2 components as the axes. Label the points with the party letter and the appropriate color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Julia\n",
    "X_Horsens = np.load('datasets/4/X_Horsens.npy')\n",
    "Y_Horsens = np.load('datasets/4/Y_Horsens.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bbaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Julia\n",
    "# build a PCA model\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X_Horsens)\n",
    "# transform the digits data onto the first two principal components\n",
    "parties_pca = pca.transform(X_Horsens)\n",
    "colors = { \"A\":\"#FF0707\",\"B\":\"#4B0082\",\"C\":\"#00FF00\",\"D\":\"#000000\",\"F\" :\"#FF00FF\",\n",
    "          \"I\":\"#00FFFF\",\"O\":\"#FFFF00\",\"V\":\"#0000FF\", \"Z\":\"#8B0000\"}\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(parties_pca[:, 0].min(), parties_pca[:, 0].max())\n",
    "plt.ylim(parties_pca[:, 1].min(), parties_pca[:, 1].max())\n",
    "for i in range(len(X_Horsens)):\n",
    "    plt.text(parties_pca[i, 0], parties_pca[i, 1], str(Y_Horsens[i]),\n",
    "             color = colors[Y_Horsens[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd744fdf",
   "metadata": {},
   "source": [
    "__(c)__ Comment on the results. You may consider the following questions for inspiration: Can the political parties be separated? Can the typical distinction of \"left-wing\" and \"right-wing\" be discerned? Which of the 18 questions (features) are most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Julia\n",
    "#to answer first two questions we will plot left-wing, right-wing and centrist parties with colours: green, blue, yellow\n",
    "#only rigth and left\n",
    "colors = { \"A\":\"#00FF00\",\"B\":\"#00FF00\",\"C\":\"#0000FF\",\"D\":\"#0000FF\",\"F\" :\"#00FF00\",\"I\":\"#0000FF\",\"O\":\"#0000FF\",\"V\":\"#0000FF\", \"Z\":\"#00FF00\"}\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(parties_pca[:, 0].min(), parties_pca[:, 0].max())\n",
    "plt.ylim(parties_pca[:, 1].min(), parties_pca[:, 1].max())\n",
    "for i in range(len(X_Horsens)):\n",
    "    plt.text(parties_pca[i, 0], parties_pca[i, 1], str(Y_Horsens[i]),\n",
    "             color = colors[Y_Horsens[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62935fe",
   "metadata": {},
   "source": [
    "On above graph we can see how leftist(green) and right-wing(blue) parties were noticably seperated based on components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cdd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#right, left and center:\n",
    "colors = { \"A\":\"#FFFF00\",\"B\":\"#FFFF00\",\"C\":\"#0000FF\",\"D\":\"#0000FF\",\"F\" :\"#00FF00\", \"I\":\"#0000FF\",\"O\":\"#0000FF\",\"V\":\"#FFFF00\", \"Z\":\"#00FF00\"}\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(parties_pca[:, 0].min(), parties_pca[:, 0].max())\n",
    "plt.ylim(parties_pca[:, 1].min(), parties_pca[:, 1].max())\n",
    "for i in range(len(X_Horsens)):\n",
    "    plt.text(parties_pca[i, 0], parties_pca[i, 1], str(Y_Horsens[i]),\n",
    "             color = colors[Y_Horsens[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0b981",
   "metadata": {},
   "source": [
    "Here we can see how the parties are seperated when we consider also centrist parties. We can see how they are seperated from left wing ones but mix with right-wing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To answer which questions are most 'important' I did pca with one component to see variance of each feature\n",
    "Xs = StandardScaler().fit_transform(X_Horsens)\n",
    "pca = decomposition.PCA(n_components=1)\n",
    "res = pca.fit_transform(Xs)\n",
    "\n",
    "print(pca.explained_variance_ratio_)#Percentage of variance explained by component.\n",
    "print(pd.DataFrame(pca.components_,columns=list(range(0, 18)),index = ['PC-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873e9b8",
   "metadata": {},
   "source": [
    "Looks like question number 10 and 14 are ones that are most important in this dataset.  \n",
    "10. Det er en god idé, at dagpengesatsen for nyuddannede er sat ned  \n",
    "14. Der bør dannes en regering hen over midten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here just plotting percentage of explained variance depending on number of components(for curiosity only)\n",
    "x = list(range(1, 1 + len(pca.explained_variance_ratio_)))\n",
    "plt.plot(x, np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19658e1",
   "metadata": {},
   "source": [
    "The number of candidates (83) is on the (very) low side when we want to do machine learning. Luckily, the neighbouring city of Databorg had no less than 8,300 candidates standing for election, with a political environment similar to that of Horsens. In the following, we will use the data from Databorg. These are stored in the numpy files `X_Databorg.npy` and `Y_Databorg.npy` in same format as the Horsens data.\n",
    "\n",
    "__(d)__ Once again, perform a Principal Component Analysis and visualize the results. Compare the results to those of the Horsens data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5adb514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Julia\n",
    "X_Databorg = np.load('datasets/4/X_Databorg.npy')\n",
    "Y_Databorg = np.load('datasets/4/Y_Databorg.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8445d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a PCA model\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X_Databorg)\n",
    "# transform the digits data onto the first two principal components\n",
    "parties_pca = pca.transform(X_Databorg)\n",
    "colors = { \"A\":\"#FF0707\",\"B\":\"#4B0082\",\"C\":\"#00FF00\",\"D\":\"#000000\",\"F\" :\"#FF00FF\",\n",
    "          \"I\":\"#00FFFF\",\"O\":\"#FFFF00\",\"V\":\"#0000FF\", \"Z\":\"#8B0000\"}\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(parties_pca[:, 0].min(), parties_pca[:, 0].max())\n",
    "plt.ylim(parties_pca[:, 1].min(), parties_pca[:, 1].max())\n",
    "for i in range(len(X_Databorg)):\n",
    "    plt.text(parties_pca[i, 0], parties_pca[i, 1], str(Y_Databorg[i]),\n",
    "             color = colors[Y_Databorg[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b642c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to answer first two questions we will plot left-wing, right-wing and centrist parties with colours: green, blue, yellow\n",
    "#only rigth and left\n",
    "colors = { \"A\":\"#00FF00\",\"B\":\"#00FF00\",\"C\":\"#0000FF\",\"D\":\"#0000FF\",\"F\" :\"#00FF00\",\"I\":\"#0000FF\",\"O\":\"#0000FF\",\"V\":\"#0000FF\", \"Z\":\"#00FF00\"}\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(parties_pca[:, 0].min(), parties_pca[:, 0].max())\n",
    "plt.ylim(parties_pca[:, 1].min(), parties_pca[:, 1].max())\n",
    "for i in range(len(X_Databorg)):\n",
    "    plt.text(parties_pca[i, 0], parties_pca[i, 1], str(Y_Databorg[i]),\n",
    "             color = colors[Y_Databorg[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3cfd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#right, left and center:\n",
    "colors = { \"A\":\"#FFFF00\",\"B\":\"#FFFF00\",\"C\":\"#0000FF\",\"D\":\"#0000FF\",\"F\" :\"#00FF00\", \"I\":\"#0000FF\",\"O\":\"#0000FF\",\"V\":\"#FFFF00\", \"Z\":\"#00FF00\"}\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(parties_pca[:, 0].min(), parties_pca[:, 0].max())\n",
    "plt.ylim(parties_pca[:, 1].min(), parties_pca[:, 1].max())\n",
    "for i in range(len(X_Databorg)):\n",
    "    plt.text(parties_pca[i, 0], parties_pca[i, 1], str(Y_Databorg[i]),\n",
    "             color = colors[Y_Databorg[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63a135",
   "metadata": {},
   "source": [
    "Confident that we can predict the partisan affiliation of a politician reasonably well based on their answers to the test, we want to build a model that will allow us to distinguish between the 9 political parties. For this purpose, we split the data into a training and a validation set.\n",
    "\n",
    "__(e)__ Split the data into a training and a validation set, with appropriate fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b82832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Julia\n",
    "X_train, X_test , y_train, y_test = train_test_split(X_Databorg, Y_Databorg,  random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc5200",
   "metadata": {},
   "source": [
    "First, we assume that a Naive Bayes approach is sufficient for our purposes.\n",
    "\n",
    "__(f)__ Comment on the basic assumption of the Naive Bayes approach. Is this a reasonable assumption for the problem at hand?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e14b3",
   "metadata": {},
   "source": [
    "Julia  \n",
    "Naive Byens assumes that given predictors are unrelated. So in this case it assumes that answer to question nr 1 is unrelated to answer to question nr 2 or any other. As it's not entirely true, for example welfare questions will be related, if one is answered negatively then it's highly probable that other one will be rated so as well. In our case we can try ignoring it, to check what results will it show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153bbd8",
   "metadata": {},
   "source": [
    "__(g)__ Classify the instances of the validation set using a Naive Bayes approach. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Julia\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "print(\"Accuracy score train: {}\".format(nb.score(X_train, y_train)))\n",
    "print(\"Accuracy score test: {}\".format(nb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f135b8",
   "metadata": {},
   "source": [
    "Accuracy for test data is higher than I expected. Model is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71c63d",
   "metadata": {},
   "source": [
    "Assume instead that a _k_-nearest neighbour approach is sufficient for our  needs.\n",
    "\n",
    "__(h)__ Using default settings of the _k_-NN classifier, classify the instances of the validation set. Comment on the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b81fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Julia\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"Model accuracy on the test data: {}\".format(knn.score(X_test,y_test)))\n",
    "print(\"Model accuracy on the train data: {}\".format(knn.score(X_train,y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3ca7d",
   "metadata": {},
   "source": [
    "Accuracy increased when using k-NN classifier. And it's just with default k. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a201e2",
   "metadata": {},
   "source": [
    "__(i)__ Play around with different values of _k_. Decide on a \"good\" value of _k_. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e62e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Julia\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "ks = range(1, 30)\n",
    "\n",
    "for k in ks:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # record training set accuracy\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    # record generalization accuracy\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(ks, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(ks, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5de4029",
   "metadata": {},
   "source": [
    "####  Looks like 15 neithbours would be optimal number in this case to avoid overfitting and get reasonable accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4dd2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"Model accuracy on the test data: {}\".format(knn.score(X_test,y_test)))\n",
    "print(\"Model accuracy on the train data: {}\".format(knn.score(X_train,y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1735c72",
   "metadata": {},
   "source": [
    "We now try to use a decision tree instead.\n",
    "\n",
    "__(j)__ What is the _minimum_ depth of an appropriate decision tree? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cd2082",
   "metadata": {},
   "source": [
    "Nikita  \n",
    "The minimum depth is the number of nodes along the shortest path from the root node down to the nearest leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1438f0",
   "metadata": {},
   "source": [
    "__(k)__ Build a decision tree with at least the depth from above. Play around with the tree depth. Include a figure that shows some relevant measure of the performance as a function of the tree depth. Comment on any issues of over-fitting. Decide on a tree which you will keep for later use. Can you do better than the _k_-NN classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9c6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nikita\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "X_Databorg = np.load('datasets/4/X_Databorg.npy')\n",
    "Y_Databorg = np.load('datasets/4/Y_Databorg.npy')\n",
    "X_Horsens = np.load('datasets/4/X_Horsens.npy')\n",
    "Y_Horsens = np.load('datasets/4/Y_Horsens.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd83f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test , y_train, y_test = train_test_split(X_Databorg, Y_Databorg, test_size=0.5, random_state=69)\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Accuracy on training data: {}\".format(clf.score(X_train, y_train)))\n",
    "print(\"Accuracy on testing data: {}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "## Example of overfitting\n",
    "\n",
    "## Unpruned trees are therefore prone to overfitting and not generalizing well to new data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Lets do Pre-prunning\n",
    "X_train, X_test , y_train, y_test = train_test_split(X_Databorg, Y_Databorg, test_size=0.5, random_state=69)\n",
    "\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "ks = range(1, 16)\n",
    "\n",
    "for k in ks:\n",
    "    clf = DecisionTreeClassifier(max_depth=k, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # record training set accuracy\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    # record generalization accuracy\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(ks, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(ks, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf528e",
   "metadata": {},
   "source": [
    "\n",
    "k-NN classifier shows better results than any decison tree with any depth\n",
    "\n",
    "The best tree with max_depth 5. Trees with max_depth higher than 5-6 have issues with overfitting as training accurance is much higher than test accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f031f805",
   "metadata": {},
   "source": [
    "__(l)__ What are the most important features? Visualize this in an appropriate way. Does it match what you would expect? Compare to the results of the PCA analysis. Do we expect them to be the same? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667989ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nikita\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "values = np.load('datasets/4/X_Databorg.npy')\n",
    "keys = np.load('datasets/4/Y_Databorg.npy')\n",
    "\n",
    "def plot_feature_importance(model):\n",
    "    n_features = values.data.shape[1]\n",
    "    plt.barh(np.arange(n_features), model, align='center')\n",
    "    plt.yticks(np.arange(n_features))\n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)\n",
    "\n",
    "plot_feature_importance(clf.feature_importances_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4770c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xs = StandardScaler().fit_transform(values)\n",
    "pca = decomposition.PCA(n_components=1)\n",
    "res = pca.fit_transform(Xs)\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "print(pca.components_)\n",
    "#plot_feature_importance(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c3ce0",
   "metadata": {},
   "source": [
    "We know that decision trees suffer from certain problems that may be solved by using decision forests.\n",
    "\n",
    "__(m)__ Build a decision forest. Play around with the number of trees in the forest. Decide on a forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f322976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nikita\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_Databorg = np.load('datasets/4/X_Databorg.npy')\n",
    "Y_Databorg = np.load('datasets/4/Y_Databorg.npy')\n",
    "\n",
    "X_train, X_test , y_train, y_test = train_test_split(X_Databorg, Y_Databorg, test_size=0.5, random_state=69)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=33, random_state=2)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model accuracy on the original/test? data: {}\".format(forest.score(X_test,y_test)))\n",
    "print(\"Model accuracy on the train data: {}\".format(forest.score(X_train,y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d460a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "ks = range(1, 50)\n",
    "\n",
    "for k in ks:\n",
    "    clf = RandomForestClassifier(n_estimators=k, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(k, clf.score(X_train, y_train), clf.score(X_test, y_test), clf.score(X_train, y_train) - clf.score(X_test, y_test))\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "   \n",
    "    \n",
    "plt.plot(ks, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(ks, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.legend();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da7963f",
   "metadata": {},
   "source": [
    "__(n)__ Extract the most important features. Comment and compare with previously obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nikita\n",
    "plot_feature_importance(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628afcd7",
   "metadata": {},
   "source": [
    "Most important features in the single tree are 9, 16, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c5826",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(forest.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759976c",
   "metadata": {},
   "source": [
    "Most important features in the forest are 9, 16, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ebd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(clf.feature_importances_) ## tree - blue\n",
    "plot_feature_importance(forest.feature_importances_) ## forest - orange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(forest.feature_importances_) ## forest - blue\n",
    "plot_feature_importance(clf.feature_importances_) ## tree - orange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119fc524",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Some of the features in the random forest become less important \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc53cac",
   "metadata": {},
   "source": [
    "Finally, we want to compare the models we have worked with so far (i.e., Naive Bayes, _k_-NN, decision tree and decision forest).\n",
    "\n",
    "__(o)__ Compare the results of the in terms of confusion matrices, accuracy, precision, recall, and f-score. How well can we predict the partisan affiliation of a candidate based on their answers to a test? How does this compare with your intuition? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Julia\n",
    "#I collected best versions of models we used and will try to compare them\n",
    "tree = DecisionTreeClassifier(max_depth=6, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "forest = RandomForestClassifier(n_estimators=32, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "#=====================================================================\n",
    "print('Naive Bayes')\n",
    "print(\"Model accuracy: {}\".format(nb.score(X_test,y_test)))\n",
    "y_pred = nb.predict(X_test) \n",
    "print(\"Precision: {}\".format(metrics.precision_score(y_test,y_pred, average='macro')))#What proportion of positive identifications was actually correct?\n",
    "print(\"Recall: {}\".format(metrics.recall_score(y_test,y_pred, average='macro')))#What proportion of actual positives was identified correctly?\n",
    "print(\"F-score: {}\".format(metrics.f1_score(y_test,y_pred, average='macro')))\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "#=====================================================================\n",
    "print('k-Neigbours classifier')\n",
    "print(\"Model accuracy: {}\".format(knn.score(X_test,y_test)))\n",
    "y_pred = knn.predict(X_test) \n",
    "print(\"Precision: {}\".format(metrics.precision_score(y_test,y_pred, average='macro')))\n",
    "print(\"Recall: {}\".format(metrics.recall_score(y_test,y_pred, average='macro')))\n",
    "print(\"F-score: {}\".format(metrics.f1_score(y_test,y_pred, average='macro')))\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "#=====================================================================\n",
    "print('Decision Tree Classifier')\n",
    "print(\"Model accuracy: {}\".format(tree.score(X_test,y_test)))\n",
    "y_pred = tree.predict(X_test) \n",
    "print(\"Precision: {}\".format(metrics.precision_score(y_test,y_pred, average='macro')))\n",
    "print(\"Recall: {}\".format(metrics.recall_score(y_test,y_pred, average='macro')))\n",
    "print(\"F-score: {}\".format(metrics.f1_score(y_test,y_pred, average='macro')))\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "#=====================================================================\n",
    "print('Random Forest Classifier')\n",
    "print(\"Model accuracy: {}\".format(forest.score(X_test,y_test)))\n",
    "y_pred = forest.predict(X_test) \n",
    "print(\"Precision: {}\".format(metrics.precision_score(y_test,y_pred, average='macro')))\n",
    "print(\"Recall: {}\".format(metrics.recall_score(y_test,y_pred, average='macro')))\n",
    "print(\"F-score: {}\".format(metrics.f1_score(y_test,y_pred, average='macro')))\n",
    "cm = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67640aa1",
   "metadata": {},
   "source": [
    "k-Neighbours classifier turned out to be the best in terms of accuracy and other metrics. We can predict kandidate's partisan affiliation with 0.82 accuracy what is way more than what I assumed in beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d1258",
   "metadata": {},
   "source": [
    "## 5. Sentiment Analysis\n",
    "\n",
    "__You should be able to do this exercise after Lecture 8.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f62c6",
   "metadata": {},
   "source": [
    "In this exercise we use the IMDb-dataset, which we will use to perform a sentiment analysis. The code below assumes that the data is placed in the same folder as this notebook. We see that the reviews are loaded as a pandas dataframe, and print the beginning of the first few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929cfecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "reviews = pd.read_csv('datasets/5/reviews.txt', header=None)\n",
    "labels = pd.read_csv('datasets/5/labels.txt', header=None)\n",
    "Y = (labels=='positive').astype(np.int_)\n",
    "\n",
    "print(type(reviews))\n",
    "print(reviews.head())\n",
    "\n",
    "#removing reviews that are not strings\n",
    "\n",
    "print(reviews.shape)\n",
    "\n",
    "#reviews = reviews[~reviews[0].str.isnumeric()]\n",
    "reviews = reviews.rename(columns={0 : \"text\"})\n",
    "print(reviews.head())\n",
    "#reviews.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f02cccd",
   "metadata": {},
   "source": [
    "**(a)** Split the reviews and labels in test, train and validation sets. The train and validation sets will be used to train your model and tune hyperparameters, the test set will be saved for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84a1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval, X_test, y_trainval, y_test = train_test_split(reviews, Y, random_state=69)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=69)\n",
    "print(\"Size of training set:{}\".format(X_train.shape[0]))\n",
    "print(\"Size of validation set:{}\".format(X_val.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af469bc",
   "metadata": {},
   "source": [
    "**(b)** Use the `CountVectorizer` from `sklearn.feature_extraction.text` to create a Bag-of-Words representation of the reviews. (See an example of how to do this in chapter 7 of \"Muller and Guido\"). Only use the 10,000 most frequent words (use the `max_features`-parameter of `CountVectorizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a61cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features = 10000)\n",
    "vect.fit(X_train['text'])\n",
    "\n",
    "X_train = vect.transform(X_train['text'])\n",
    "X_test = vect.transform(X_test['text'])\n",
    "X_val = vect.transform(X_val['text'])\n",
    "\n",
    "X_train = pd.DataFrame(X_train.toarray(),columns = vect.get_feature_names())\n",
    "X_val = pd.DataFrame(X_val.toarray(),columns = vect.get_feature_names())\n",
    "X_test = pd.DataFrame(X_test.toarray(),columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd37af",
   "metadata": {},
   "source": [
    "**(c)** Explore the representation of the reviews. How is a single word represented? How about a whole review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9214fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.head())\n",
    "print(X_test.head())\n",
    "print(X_val.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fadc2c",
   "metadata": {},
   "source": [
    "Single word is a feature that is if present in review has value 1 and if not then 0. Number of words was limited to 10000 most common words present in training set. Therefore one review has 10000 features now.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfdabb7",
   "metadata": {},
   "source": [
    "**(d)** Train a neural network with a single hidden layer on the dataset, tuning the relevant hyperparameters to optimize accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12437985",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() #initialize a neural network model\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "input_size = X_train.shape[0]\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "X_val = np.asarray(X_val)\n",
    "y_val = np.asarray(y_val)\n",
    "\n",
    "model.add(Dense(10, activation='relu', input_dim=input_dim)) #hidden layer #kernel_regularizer = regularizers.l2(0.001)\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid')) #output layer\n",
    "\n",
    "#sgd = optimizers.SGD(learning_rate = 0.1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 2, batch_size = 50, verbose = 0,validation_data=(X_val, y_val))#, validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a918c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36d55d2",
   "metadata": {},
   "source": [
    "**(e)** Test your sentiment-classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c03dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "print(\"Loss + accuracy on train data: {}\".format(model.evaluate(X_train, y_train)))\n",
    "print(\"Loss + accuracy on test data: {}\".format(model.evaluate(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae2b46",
   "metadata": {},
   "source": [
    "**(h)** Use the classifier to classify a few sentences you write yourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"teribble movie, I loved it\",\"Excellent movie, I hated it!\",\"Awful movie, I regret I saw it\",\"I will watch it again and again\"]\n",
    "#sentences = pd.DataFrame(sentences,columns = vect.get_feature_names())\n",
    "sentences = vect.transform(sentences)\n",
    "\n",
    "model.predict(sentences) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41680d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = input()\n",
    "for i in a:\n",
    "    print(int(i)**2, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63bd08",
   "metadata": {},
   "source": [
    "## 6. Speech Recognition\n",
    "\n",
    "__You should be able to do this exercise after Lecture 9.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d260e",
   "metadata": {},
   "source": [
    "In this exercise, we will work with the <a href=\"https://arxiv.org/pdf/1804.03209.pdf\">Google Speech Command Dataset</a>, which can be downloaded from <a href=\"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\">here</a> (note: you do not need to download the full dataset, but it will allow you to play around with the raw audiofiles). This dataset contains 105,829 one-second long audio files with utterances of 35 common words.\n",
    "\n",
    "We will use a subset of this dataset as indicated in the table below.\n",
    "\n",
    "| Word | How many? | Class # |\n",
    "| :-: | :-: | :-: |\n",
    "| Yes | 4,044 | 3 |\n",
    "| No | 3,941 | 1 |\n",
    "| Stop | 3,872 | 2 |\n",
    "| Go | 3,880 | 0 |\n",
    "\n",
    "The data is given in the files `XSound.npy` and `YSound.npy`, both of which can be imported using `numpy.load`. `XSound.npy` contains spectrograms (_e.g._, matrices with a time-axis and a frequency-axis of size 62 (time) x 65 (frequency)). `YSound.npy` contains the class number, as indicated in the table above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4736285",
   "metadata": {},
   "source": [
    "__(a)__ Explore and prepare the data, including splitting the data in training, validation and testing data, handling outliers, perhaps taking logarithms, etc. Data preparation is - as always - quite important. Document what you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37503665",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install h5py\n",
    "pip install typing-extensions\n",
    "pip install wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ba0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow --no-cache-dir --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io.wavfile import read, write\n",
    "from scipy import signal\n",
    "from IPython.display import Audio\n",
    "from numpy.fft import fft, ifft, fftfreq\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "XSound = np.load('datasets/6/XSound.npy')[:500]\n",
    "YSound = np.load('datasets/6/YSound.npy')[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7930b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(YSound)\n",
    "dataPoints = XSound.shape[0]\n",
    "timeDim = XSound.shape[1]\n",
    "freqDim = XSound.shape[2]\n",
    "features = timeDim * freqDim\n",
    "\n",
    "print(classes)\n",
    "\n",
    "XSound_flat = XSound.reshape(-1)\n",
    "minimum = np.min(XSound_flat)\n",
    "maximum = np.max(XSound_flat)\n",
    "\n",
    "print(\"min:\",minimum,\"\\nmaxinum:\", maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea301f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scale these values so they range between 0 and 1\n",
    "def Scale(_min, _max, _val):\n",
    "    return (_val - _min) / (_max - _min);\n",
    "\n",
    "for i, X in enumerate(XSound):\n",
    "    X_flat = X.reshape(timeDim * freqDim, -1)\n",
    "\n",
    "    X_min = np.min(X_flat)\n",
    "    X_max = np.max(X_flat)\n",
    "    \n",
    "    X_scaled = []\n",
    "    for X_measure in X_flat:\n",
    "        X_scaled.append(Scale(X_min, X_max, X_measure))\n",
    "        \n",
    "    XSound[i] = np.array(X_scaled).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSound_flat = XSound.reshape(-1)\n",
    "minimum = np.min(XSound_flat)\n",
    "maximum = np.max(XSound_flat)\n",
    "\n",
    "print(\"min:\",minimum,\"\\nmaxinum:\", maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed2ab5",
   "metadata": {},
   "source": [
    "__(b)__ Visualize a few examples of yes's, no's, stop's and go's, so that you have a reasonable intuitive understanding of the difference between the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6569acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "sample = XSound[idx]\n",
    "\n",
    "#display(sample)\n",
    "\n",
    "plt.specgram(sample, fs = features)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "\n",
    "plt.ylabel('Frequency') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceac378",
   "metadata": {},
   "source": [
    "__(c)__ Train a neural network and at least one other algorithm on the data. Find a good set of hyperparameters for each model. Do you think a neural network is suitable for this kind of problem? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03362aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3,3), activation='tanh', input_shape=XSound[0].shape)) #32 feature maps, 3*3 local receptive fields\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 4, activation = 'softmax')) #fully connected output layer\n",
    "\n",
    "sgd = optimizers.SGD(learning_rate = 0.1)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = 10, batch_size = 50, verbose = 1, validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4c4f8f",
   "metadata": {},
   "source": [
    "__(d)__ Classify instances of the validation set using your models. Comment on the results in terms of metrics you have learned in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb3c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test, Y_test, epochs = 10, batch_size = 50, verbose = 1, validation_split = 0.3)\n",
    "# o ex 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5689b4",
   "metadata": {},
   "source": [
    "__(e)__ Identify (a few) misclassified words, including what they are misclassified as. Visualize them as before, and compare with your intuitive understanding of how the words look. Do you find the misclassified examples surprising?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bd8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f5e76",
   "metadata": {},
   "source": [
    "## 7. Group Assignment & Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db549c55",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__You should be able to start up on this exercise after Lecture 1.__\n",
    "\n",
    "*This exercise must be a group effort. That means everyone must participate in the assignment.*\n",
    "\n",
    "In this assignment you will solve a data science problem end-to-end, pretending to be recently hired data scientists in a company. To help you get started, we've prepared a checklist to guide you through the project. Here are the main steps that you will go through:\n",
    "\n",
    "1. Frame the problem and look at the big picture\n",
    "2. Get the data\n",
    "3. Explore and visualise the data to gain insights\n",
    "4. Prepare the data to better expose the underlying data patterns to machine learning algorithms\n",
    "5. Explore many different models and short-list the best ones\n",
    "6. Fine-tune your models\n",
    "7. Present your solution \n",
    "\n",
    "In each step we list a set of questions that one should have in mind when undertaking a data science project. The list is not meant to be exhaustive, but does contain a selection of the most important questions to ask. We will be available to provide assistance with each of the steps, and will allocate some part of each lesson towards working on the projects.\n",
    "\n",
    "Your group must submit a _**single**_ Jupyter notebook, structured in terms of the first 6 sections listed above (the seventh will be a video uploaded to some streaming platform, e.g. YouTube, Vimeo, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85806b",
   "metadata": {},
   "source": [
    "### 1. Analysis: Frame the problem and look at the big picture\n",
    "1. Find a problem/task that everyone in the group finds interesting\n",
    "2. Define the objective in business terms\n",
    "3. How should you frame the problem (supervised/unsupervised etc.)?\n",
    "4. How should performance be measured?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf807c0",
   "metadata": {},
   "source": [
    "### 2. Get the data\n",
    "1. Find and document where you can get the data from\n",
    "2. Get the data\n",
    "3. Check the size and type of data (time series, geographical etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289787da",
   "metadata": {},
   "source": [
    "### 3. Explore the data\n",
    "1. Create a copy of the data for explorations (sampling it down to a manageable size if necessary)\n",
    "2. Create a Jupyter notebook to keep a record of your data exploration\n",
    "3. Study each feature and its characteristics:\n",
    "    * Name\n",
    "    * Type (categorical, int/float, bounded/unbounded, text, structured, etc)\n",
    "    * Percentage of missing values\n",
    "    * Check for outliers, rounding errors etc\n",
    "4. For supervised learning tasks, identify the target(s)\n",
    "5. Visualise the data\n",
    "6. Study the correlations between features\n",
    "7. Identify the promising transformations you may want to apply (e.g. convert skewed targets to normal via a log transformation)\n",
    "8. Document what you have learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6442d914",
   "metadata": {},
   "source": [
    "### 4. Prepare the data\n",
    "Notes:\n",
    "* Work on copies of the data (keep the original dataset intact).\n",
    "* Write functions for all data transformations you apply, for three reasons:\n",
    "    * So you can easily prepare the data the next time you run your code\n",
    "    * So you can apply these transformations in future projects\n",
    "    * To clean and prepare the test set\n",
    "    \n",
    "    \n",
    "1. Data cleaning:\n",
    "    * Fix or remove outliers (or keep them)\n",
    "    * Fill in missing values (e.g. with zero, mean, median, regression ...) or drop their rows (or columns)\n",
    "2. Feature selection (optional):\n",
    "    * Drop the features that provide no useful information for the task (e.g. a customer ID is usually useless for modelling).\n",
    "3. Feature engineering, where appropriate:\n",
    "    * Discretize continuous features\n",
    "    * Use one-hot encoding if/when relevant\n",
    "    * Add promising transformations of features (e.g. $\\log(x)$, $\\sqrt{x}$, $x^2$, etc)\n",
    "    * Aggregate features into promising new features\n",
    "4. Feature scaling: standardise or normalise features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e836f5e",
   "metadata": {},
   "source": [
    "### 5. Short-list promising models\n",
    "We expect you to do some additional research and train at **least one model per team member**.\n",
    "\n",
    "1. Train mainly quick and dirty models from different categories (e.g. linear, SVM, Random Forests etc) using default parameters\n",
    "2. Measure and compare their performance\n",
    "3. Analyse the most significant variables for each algorithm\n",
    "4. Analyse the types of errors the models make\n",
    "5. Have a quick round of feature selection and engineering if necessary\n",
    "6. Have one or two more quick iterations of the five previous steps\n",
    "7. Short-list the top three to five most promising models, preferring models that make different types of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fdb683",
   "metadata": {},
   "source": [
    "### 6. Fine-tune the system\n",
    "1. Fine-tune the hyperparameters\n",
    "2. Once you are confident about your final model, measure its performance on the test set to estimate the generalisation error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c46de",
   "metadata": {},
   "source": [
    "### 7. Present your solution\n",
    "1. Document what you have done\n",
    "2. Create a nice 15 minute video presentation with slides\n",
    "    * Make sure you highlight the big picture first\n",
    "3. Explain why your solution achieves the business objective\n",
    "4. Don't forget to present interesting points you noticed along the way:\n",
    "    * Describe what worked and what did not\n",
    "    * List your assumptions and you model's limitations\n",
    "5. Ensure your key findings are communicated through nice visualisations or easy-to-remember statements (e.g. \"the median income is the number-one predictor of housing prices\")\n",
    "6. Upload the presentation to some online platform, e.g. YouTube or Vimeo, and supply a link to the video in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725eef98",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a382ea",
   "metadata": {},
   "source": [
    "Géron, A. 2017, *Hands-On Machine Learning with Scikit-Learn and Tensorflow*, Appendix B, O'Reilly Media, Inc., Sebastopol.\n",
    "Wunderground.com. 2022. The Highest Anemometer-Measured Wind Speeds on Earth. Available at: <https://www.wunderground.com/cat6/the-highest-anemometer-measured-wind-speeds-on-earth#:~:text=253%20mph%20%E2%80%A2%20Barrow%20Island%2C%20Australia%20%E2%80%A2%20April%2010%2C%201996&text=Wind%20trace%20taken%20at%20Barrow,6%3A15%20pm%20local%20time.> [Accessed 11 October 2022]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
